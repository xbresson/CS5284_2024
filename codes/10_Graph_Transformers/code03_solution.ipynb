{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfFWcbbExXcc"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra) -- Solution\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1730637248755,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "IZCvd1fTxXce",
    "outputId": "41ceed4f-96e9-4b1a-a395-f72c3621d79d"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y9hiy25BxXcf"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Es6_zDxXcf"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1730637253097,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fl68-dJTxXcf",
    "outputId": "7f142b87-0e09-4434-d76b-18d370ec5f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 0.9380s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types\n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTYjHxtUxXcg"
   },
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1730637255093,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "TQp3RdpAxXcg",
    "outputId": "19126433-8031-4aa7-d982-1cd2d516a142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node positional encoding features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature\n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrFXYNexXcg"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1730637255907,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "i77JNkdBxXcg",
    "outputId": "a2e5b1cf-7e62-4fe9-8653-d1fb55bab668"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R14LMFIRxXch"
   },
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "d7ZXaxwKxXch",
    "outputId": "af206596-e7cd-41ce-9b51-20b2b9e4167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=88, num_edges=190,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[-0.8888],\n",
      "        [-1.8995],\n",
      "        [ 1.4673],\n",
      "        [-1.2200],\n",
      "        [-0.6535],\n",
      "        [ 0.5994],\n",
      "        [-1.0217],\n",
      "        [-2.1666],\n",
      "        [-0.5883],\n",
      "        [ 0.5479]])\n",
      "batch_x: torch.Size([88])\n",
      "batch_pe: torch.Size([88, 3])\n",
      "batch_e: torch.Size([190])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBKKjxWxXch"
   },
   "source": [
    "# Exercise 1: Design the class of GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqI7VXd9gchf"
   },
   "source": [
    "### Question 1.1: Implement a Graph Multi-Head Attention (MHA) Layer with edge features\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute bi-linear products with edge feature: $q_i^T * diag(e_{ij}) * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j, edges.data[]``` form ```ij```\".  \n",
    "\n",
    "    - *Step 1.2* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V```.\n",
    "\n",
    "    - *Step 1.4:* Compute edge feature: $q_i^T * diag(e_{ij}) * k_j$.\n",
    "\n",
    "    - *Step 1.5:* Update edge feature.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V, E, F, G```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V', 'E', 'F', 'G'``` for ```g.ndata[]``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XQ-OgcDJ7rC4"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WF = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WG = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute bi-linear products with edge feature\n",
    "        qikj = (edges.src['K'] * edges.data['E'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "\n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) )\n",
    "\n",
    "        # Step 1.3: Obtain vj\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "\n",
    "        # Step 1.4: Compute edge feature: e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        eij = edges.src['K'] * edges.data['E'] * edges.dst['Q'] / torch.sqrt(torch.tensor(self.head_hidden_dim))\n",
    "\n",
    "        # Step 1.5: Update edge feature\n",
    "        edges.data['e'] = eij\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij = nodes.mailbox['expij']\n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj = nodes.mailbox['vj']\n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 )\n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator = torch.sum( expij, dim=1 )\n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "        F = self.WF(h) # size=(N, d)\n",
    "        G = self.WG(h) # size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.ndata['F'] = F.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['G'] = G.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJix5TqbrFhR"
   },
   "source": [
    "### Question 1.2: Implement a Graph Transformer layer (with edge feature)\n",
    "\n",
    "- Implement dropout, layer normalization, and residual connection layers for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Roe4rKU371mH"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Dropout layers for edge features\n",
    "        self.dropout_e_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout) # dropout value\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "\n",
    "        # Layer normalization for edge features\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # layer normalization for edge features, size=(N, d)\n",
    "        e = self.layer_norm1e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Update for edge features\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h, e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rST8IpB2rYqK"
   },
   "source": [
    "### Question 1.3: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network (with edge feature)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Adding a input edge embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer (with edge feature):* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2DQRl2bYxXch"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Initialize a edge embedding layer\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe, e):\n",
    "\n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement teh edge embedding layer\n",
    "        # size=(num_edges, hidden_dim)\n",
    "        e = self.embedding_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qzWVgqKW7vSU",
    "outputId": "3703d3bb-d48e-43d6-ed06-8723e11cb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WF): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WG): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearn/miniconda3/envs/gnn_course_gpu/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0gZzixzxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291348,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Hw7LEG5exXci",
    "outputId": "ead6b872-7372-4f0c-cffa-845c8147baff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 799233 (0.80 million)\n",
      "Epoch 0, time 1.3774, train_loss: 1.3052, test_loss: 1.1935\n",
      "Epoch 1, time 2.5902, train_loss: 1.1303, test_loss: 1.0560\n",
      "Epoch 2, time 3.8541, train_loss: 1.0228, test_loss: 1.0446\n",
      "Epoch 3, time 5.1012, train_loss: 1.0122, test_loss: 0.9710\n",
      "Epoch 4, time 6.3636, train_loss: 0.9584, test_loss: 0.9781\n",
      "Epoch 5, time 7.6143, train_loss: 0.9533, test_loss: 0.9147\n",
      "Epoch 6, time 8.8321, train_loss: 0.9424, test_loss: 0.9607\n",
      "Epoch 7, time 9.9320, train_loss: 0.9489, test_loss: 0.9130\n",
      "Epoch 8, time 11.0301, train_loss: 0.9095, test_loss: 0.9405\n",
      "Epoch 9, time 12.1909, train_loss: 0.8869, test_loss: 0.8864\n",
      "Epoch 10, time 13.3653, train_loss: 0.8649, test_loss: 0.8940\n",
      "Epoch 11, time 14.5210, train_loss: 0.8670, test_loss: 0.8543\n",
      "Epoch 12, time 15.7060, train_loss: 0.8467, test_loss: 0.8465\n",
      "Epoch 13, time 16.8138, train_loss: 0.8310, test_loss: 0.8092\n",
      "Epoch 14, time 18.0895, train_loss: 0.8281, test_loss: 0.8850\n",
      "Epoch 15, time 19.1866, train_loss: 0.8166, test_loss: 0.8222\n",
      "Epoch 16, time 20.2756, train_loss: 0.8040, test_loss: 0.8055\n",
      "Epoch 17, time 21.3660, train_loss: 0.8098, test_loss: 0.8151\n",
      "Epoch 18, time 22.4550, train_loss: 0.7866, test_loss: 0.8114\n",
      "Epoch 19, time 23.5426, train_loss: 0.7721, test_loss: 0.8997\n",
      "Epoch 20, time 24.6333, train_loss: 0.8147, test_loss: 0.8691\n",
      "Epoch 21, time 25.7367, train_loss: 0.7812, test_loss: 0.8786\n",
      "Epoch 22, time 26.8819, train_loss: 0.7474, test_loss: 0.8138\n",
      "Epoch 23, time 28.0599, train_loss: 0.7453, test_loss: 0.8190\n",
      "Epoch 24, time 29.2552, train_loss: 0.7653, test_loss: 0.8334\n",
      "Epoch 25, time 30.4142, train_loss: 0.7485, test_loss: 0.7811\n",
      "Epoch 26, time 31.5862, train_loss: 0.7167, test_loss: 0.8042\n",
      "Epoch 27, time 32.7316, train_loss: 0.7224, test_loss: 0.7892\n",
      "Epoch 28, time 33.9079, train_loss: 0.7109, test_loss: 0.8443\n",
      "Epoch 29, time 35.1154, train_loss: 0.7307, test_loss: 0.8311\n",
      "Epoch 30, time 36.3241, train_loss: 0.7296, test_loss: 0.8044\n",
      "Epoch 31, time 37.5434, train_loss: 0.7096, test_loss: 0.8179\n",
      "Epoch 32, time 38.7594, train_loss: 0.6822, test_loss: 0.8015\n",
      "Epoch 33, time 39.9702, train_loss: 0.6752, test_loss: 0.8058\n",
      "Epoch 34, time 41.1894, train_loss: 0.6893, test_loss: 0.7623\n",
      "Epoch 35, time 42.4056, train_loss: 0.6607, test_loss: 0.7696\n",
      "Epoch 36, time 43.6315, train_loss: 0.6591, test_loss: 0.7807\n",
      "Epoch 37, time 44.8592, train_loss: 0.6476, test_loss: 0.7673\n",
      "Epoch 38, time 46.0813, train_loss: 0.6425, test_loss: 0.7545\n",
      "Epoch 39, time 47.2822, train_loss: 0.6354, test_loss: 0.7741\n",
      "Epoch 40, time 48.4901, train_loss: 0.6553, test_loss: 0.7716\n",
      "Epoch 41, time 49.6547, train_loss: 0.6732, test_loss: 0.7641\n",
      "Epoch 42, time 50.8078, train_loss: 0.6621, test_loss: 0.7539\n",
      "Epoch 43, time 51.9698, train_loss: 0.6268, test_loss: 0.7790\n",
      "Epoch 44, time 53.1464, train_loss: 0.6241, test_loss: 0.7485\n",
      "Epoch 45, time 54.3568, train_loss: 0.6246, test_loss: 0.7512\n",
      "Epoch 46, time 55.6680, train_loss: 0.6114, test_loss: 0.7571\n",
      "Epoch 47, time 56.8153, train_loss: 0.6054, test_loss: 0.7359\n",
      "Epoch 48, time 57.9511, train_loss: 0.6012, test_loss: 0.7657\n",
      "Epoch 49, time 59.0957, train_loss: 0.5984, test_loss: 0.7518\n",
      "Epoch 50, time 60.1939, train_loss: 0.5973, test_loss: 0.7538\n",
      "Epoch 51, time 61.3528, train_loss: 0.5833, test_loss: 0.7309\n",
      "Epoch 52, time 62.4878, train_loss: 0.5873, test_loss: 0.7975\n",
      "Epoch 53, time 63.7194, train_loss: 0.5949, test_loss: 0.7476\n",
      "Epoch 54, time 64.9695, train_loss: 0.5761, test_loss: 0.7411\n",
      "Epoch 55, time 66.1632, train_loss: 0.5820, test_loss: 0.7681\n",
      "Epoch 56, time 67.3768, train_loss: 0.5999, test_loss: 0.7449\n",
      "Epoch 57, time 68.5791, train_loss: 0.5983, test_loss: 0.7941\n",
      "Epoch 58, time 69.7813, train_loss: 0.5893, test_loss: 0.7710\n",
      "Epoch 59, time 70.9863, train_loss: 0.5865, test_loss: 0.7546\n",
      "Epoch 60, time 72.1848, train_loss: 0.5807, test_loss: 0.7542\n",
      "Epoch 61, time 73.3879, train_loss: 0.5661, test_loss: 0.7470\n",
      "Epoch 62, time 74.5854, train_loss: 0.5761, test_loss: 0.7608\n",
      "Epoch 63, time 75.7845, train_loss: 0.5476, test_loss: 0.7547\n",
      "Epoch 64, time 76.9902, train_loss: 0.5511, test_loss: 0.7531\n",
      "Epoch 65, time 78.1773, train_loss: 0.5399, test_loss: 0.7647\n",
      "Epoch 66, time 79.3458, train_loss: 0.5544, test_loss: 0.8010\n",
      "Epoch 67, time 80.6257, train_loss: 0.5493, test_loss: 0.7471\n",
      "Epoch 68, time 81.9455, train_loss: 0.5427, test_loss: 0.7629\n",
      "Epoch 69, time 83.2582, train_loss: 0.5412, test_loss: 0.7304\n",
      "Epoch 70, time 84.5755, train_loss: 0.5388, test_loss: 0.7785\n",
      "Epoch 71, time 85.8976, train_loss: 0.5445, test_loss: 0.7375\n",
      "Epoch 72, time 87.2140, train_loss: 0.5170, test_loss: 0.7527\n",
      "Epoch 73, time 88.4979, train_loss: 0.5499, test_loss: 0.7677\n",
      "Epoch 74, time 89.6821, train_loss: 0.5318, test_loss: 0.7727\n",
      "Epoch 75, time 90.9004, train_loss: 0.5292, test_loss: 0.7413\n",
      "Epoch 76, time 92.0807, train_loss: 0.5104, test_loss: 0.7683\n",
      "Epoch 77, time 93.2651, train_loss: 0.5232, test_loss: 0.7418\n",
      "Epoch 78, time 94.4361, train_loss: 0.5277, test_loss: 0.7994\n",
      "Epoch 79, time 95.5990, train_loss: 0.5129, test_loss: 0.7781\n",
      "Epoch 80, time 96.7744, train_loss: 0.5241, test_loss: 0.7492\n",
      "Epoch 81, time 97.9685, train_loss: 0.5111, test_loss: 0.7436\n",
      "Epoch 82, time 99.1540, train_loss: 0.4985, test_loss: 0.7633\n",
      "Epoch 83, time 100.3502, train_loss: 0.5013, test_loss: 0.7658\n",
      "Epoch 84, time 101.5495, train_loss: 0.4877, test_loss: 0.7552\n",
      "Epoch 85, time 102.7431, train_loss: 0.4976, test_loss: 0.7539\n",
      "Epoch 86, time 103.9397, train_loss: 0.4722, test_loss: 0.7485\n",
      "Epoch 87, time 105.1311, train_loss: 0.4681, test_loss: 0.7398\n",
      "Epoch 88, time 106.3161, train_loss: 0.4676, test_loss: 0.7877\n",
      "Epoch 89, time 107.5338, train_loss: 0.4916, test_loss: 0.7569\n",
      "Epoch 90, time 108.7243, train_loss: 0.4876, test_loss: 0.7398\n",
      "Epoch 91, time 109.9183, train_loss: 0.4813, test_loss: 0.7460\n",
      "Epoch 92, time 111.1228, train_loss: 0.4562, test_loss: 0.7476\n",
      "Epoch 93, time 112.3264, train_loss: 0.4541, test_loss: 0.7361\n",
      "Epoch 94, time 113.5354, train_loss: 0.4598, test_loss: 0.7581\n",
      "Epoch 95, time 114.7332, train_loss: 0.4605, test_loss: 0.7653\n",
      "Epoch 96, time 115.9264, train_loss: 0.4797, test_loss: 0.7642\n",
      "Epoch 97, time 117.1408, train_loss: 0.4476, test_loss: 0.7773\n",
      "Epoch 98, time 118.3414, train_loss: 0.4695, test_loss: 0.7606\n",
      "Epoch 99, time 119.5804, train_loss: 0.4468, test_loss: 0.7457\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCjHW1YxXci"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amo_4YiKxXci"
   },
   "source": [
    "# GT without edge features\n",
    "\n",
    "Node update equation:\n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BKceFO_W8HBf"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        # Compute the dot products q_i^T * k_j\n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\"\n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lWOIc6W_8JY4"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_Gaf_Vi8MQt"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe):\n",
    "\n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "rPtk6EjKxXci",
    "outputId": "8db1c7ef-bbc3-4f67-df10-3ce74c4fac93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jt8BzHJxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168398,
     "status": "ok",
     "timestamp": 1730637716334,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xAdlIhxXxXci",
    "outputId": "ec39dc6a-0677-4b9e-93a0-d79426d21d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 0.9314, train_loss: 1.3168, test_loss: 1.2499\n",
      "Epoch 1, time 1.8029, train_loss: 1.2143, test_loss: 1.2025\n",
      "Epoch 2, time 2.6769, train_loss: 1.1688, test_loss: 1.2796\n",
      "Epoch 3, time 3.5380, train_loss: 1.1578, test_loss: 1.1455\n",
      "Epoch 4, time 4.3642, train_loss: 1.1041, test_loss: 1.1308\n",
      "Epoch 5, time 5.1937, train_loss: 1.0857, test_loss: 1.1057\n",
      "Epoch 6, time 6.0405, train_loss: 1.0807, test_loss: 1.1218\n",
      "Epoch 7, time 6.9026, train_loss: 1.0703, test_loss: 1.0848\n",
      "Epoch 8, time 7.7701, train_loss: 1.0747, test_loss: 1.1412\n",
      "Epoch 9, time 8.6346, train_loss: 1.0751, test_loss: 1.1133\n",
      "Epoch 10, time 9.5016, train_loss: 1.0573, test_loss: 1.0613\n",
      "Epoch 11, time 10.3364, train_loss: 1.0259, test_loss: 1.0305\n",
      "Epoch 12, time 11.1673, train_loss: 0.9963, test_loss: 1.0270\n",
      "Epoch 13, time 12.0422, train_loss: 0.9911, test_loss: 1.0027\n",
      "Epoch 14, time 12.8676, train_loss: 0.9720, test_loss: 1.0605\n",
      "Epoch 15, time 13.6956, train_loss: 0.9566, test_loss: 0.9541\n",
      "Epoch 16, time 14.6380, train_loss: 0.9485, test_loss: 1.0249\n",
      "Epoch 17, time 15.4643, train_loss: 0.9563, test_loss: 1.0068\n",
      "Epoch 18, time 16.2898, train_loss: 0.9364, test_loss: 0.9570\n",
      "Epoch 19, time 17.1146, train_loss: 0.9057, test_loss: 0.9687\n",
      "Epoch 20, time 17.9589, train_loss: 0.9257, test_loss: 0.9670\n",
      "Epoch 21, time 18.8513, train_loss: 0.8919, test_loss: 0.9391\n",
      "Epoch 22, time 19.6861, train_loss: 0.8936, test_loss: 0.9465\n",
      "Epoch 23, time 20.5455, train_loss: 0.8950, test_loss: 0.9350\n",
      "Epoch 24, time 21.3901, train_loss: 0.8974, test_loss: 0.9953\n",
      "Epoch 25, time 22.2164, train_loss: 0.9151, test_loss: 0.9065\n",
      "Epoch 26, time 23.0426, train_loss: 0.8906, test_loss: 0.9368\n",
      "Epoch 27, time 23.8720, train_loss: 0.8851, test_loss: 0.9138\n",
      "Epoch 28, time 24.6990, train_loss: 0.8974, test_loss: 0.9495\n",
      "Epoch 29, time 25.5986, train_loss: 0.8725, test_loss: 0.9108\n",
      "Epoch 30, time 26.4260, train_loss: 0.8736, test_loss: 0.9524\n",
      "Epoch 31, time 27.2552, train_loss: 0.8543, test_loss: 0.9258\n",
      "Epoch 32, time 28.1559, train_loss: 0.8541, test_loss: 0.9083\n",
      "Epoch 33, time 28.9956, train_loss: 0.8593, test_loss: 0.9102\n",
      "Epoch 34, time 29.8617, train_loss: 0.8923, test_loss: 0.9541\n",
      "Epoch 35, time 30.7138, train_loss: 0.8701, test_loss: 0.9302\n",
      "Epoch 36, time 31.5389, train_loss: 0.8899, test_loss: 0.9814\n",
      "Epoch 37, time 32.3596, train_loss: 0.8667, test_loss: 0.9133\n",
      "Epoch 38, time 33.1815, train_loss: 0.8949, test_loss: 0.9092\n",
      "Epoch 39, time 34.0500, train_loss: 0.8828, test_loss: 0.9905\n",
      "Epoch 40, time 34.9205, train_loss: 0.8647, test_loss: 1.0199\n",
      "Epoch 41, time 35.7912, train_loss: 0.8541, test_loss: 0.9308\n",
      "Epoch 42, time 36.7061, train_loss: 0.8398, test_loss: 0.9774\n",
      "Epoch 43, time 37.6936, train_loss: 0.8375, test_loss: 0.9249\n",
      "Epoch 44, time 38.5384, train_loss: 0.8435, test_loss: 0.9270\n",
      "Epoch 45, time 39.4097, train_loss: 0.8168, test_loss: 0.9045\n",
      "Epoch 46, time 40.2766, train_loss: 0.8189, test_loss: 0.9122\n",
      "Epoch 47, time 41.1479, train_loss: 0.8626, test_loss: 0.9293\n",
      "Epoch 48, time 42.0226, train_loss: 0.8675, test_loss: 0.9147\n",
      "Epoch 49, time 42.8933, train_loss: 0.8349, test_loss: 0.9574\n",
      "Epoch 50, time 43.7774, train_loss: 0.8402, test_loss: 0.9352\n",
      "Epoch 51, time 44.6546, train_loss: 0.8381, test_loss: 0.9217\n",
      "Epoch 52, time 45.5381, train_loss: 0.8126, test_loss: 0.9493\n",
      "Epoch 53, time 46.4211, train_loss: 0.8076, test_loss: 0.9230\n",
      "Epoch 54, time 47.3031, train_loss: 0.8204, test_loss: 0.9841\n",
      "Epoch 55, time 48.1846, train_loss: 0.8236, test_loss: 0.9448\n",
      "Epoch 56, time 49.0672, train_loss: 0.7963, test_loss: 0.9347\n",
      "Epoch 57, time 49.9486, train_loss: 0.7939, test_loss: 0.9367\n",
      "Epoch 58, time 50.8276, train_loss: 0.8004, test_loss: 0.9093\n",
      "Epoch 59, time 51.7106, train_loss: 0.7944, test_loss: 0.9335\n",
      "Epoch 60, time 52.5886, train_loss: 0.7819, test_loss: 0.9339\n",
      "Epoch 61, time 53.4702, train_loss: 0.7851, test_loss: 0.9819\n",
      "Epoch 62, time 54.3514, train_loss: 0.8019, test_loss: 0.9254\n",
      "Epoch 63, time 55.2352, train_loss: 0.7896, test_loss: 0.9491\n",
      "Epoch 64, time 56.1686, train_loss: 0.8163, test_loss: 0.9514\n",
      "Epoch 65, time 57.0512, train_loss: 0.8073, test_loss: 0.9368\n",
      "Epoch 66, time 57.9322, train_loss: 0.7877, test_loss: 0.9361\n",
      "Epoch 67, time 58.8088, train_loss: 0.7896, test_loss: 0.9742\n",
      "Epoch 68, time 59.6782, train_loss: 0.7823, test_loss: 0.9091\n",
      "Epoch 69, time 60.5394, train_loss: 0.7757, test_loss: 0.9555\n",
      "Epoch 70, time 61.4207, train_loss: 0.7746, test_loss: 0.9387\n",
      "Epoch 71, time 62.2824, train_loss: 0.7633, test_loss: 0.9349\n",
      "Epoch 72, time 63.1554, train_loss: 0.7517, test_loss: 0.9313\n",
      "Epoch 73, time 64.0189, train_loss: 0.7620, test_loss: 0.9323\n",
      "Epoch 74, time 64.8937, train_loss: 0.7582, test_loss: 0.9717\n",
      "Epoch 75, time 65.7875, train_loss: 0.7640, test_loss: 0.9310\n",
      "Epoch 76, time 66.6485, train_loss: 0.7402, test_loss: 0.9587\n",
      "Epoch 77, time 67.5227, train_loss: 0.7415, test_loss: 0.9183\n",
      "Epoch 78, time 68.4018, train_loss: 0.7394, test_loss: 0.9782\n",
      "Epoch 79, time 69.2742, train_loss: 0.7387, test_loss: 0.9218\n",
      "Epoch 80, time 70.1500, train_loss: 0.7279, test_loss: 0.9174\n",
      "Epoch 81, time 71.0238, train_loss: 0.7249, test_loss: 0.9477\n",
      "Epoch 82, time 71.8923, train_loss: 0.7264, test_loss: 0.9683\n",
      "Epoch 83, time 72.7505, train_loss: 0.7297, test_loss: 0.9936\n",
      "Epoch 84, time 73.6272, train_loss: 0.7204, test_loss: 0.9578\n",
      "Epoch 85, time 74.5064, train_loss: 0.7199, test_loss: 0.9405\n",
      "Epoch 86, time 75.3789, train_loss: 0.7156, test_loss: 0.9618\n",
      "Epoch 87, time 76.2563, train_loss: 0.6950, test_loss: 0.9487\n",
      "Epoch 88, time 77.1333, train_loss: 0.7248, test_loss: 0.9427\n",
      "Epoch 89, time 78.0050, train_loss: 0.7216, test_loss: 0.9539\n",
      "Epoch 90, time 78.8864, train_loss: 0.7208, test_loss: 0.9375\n",
      "Epoch 91, time 79.7646, train_loss: 0.7029, test_loss: 0.9713\n",
      "Epoch 92, time 80.6352, train_loss: 0.6871, test_loss: 0.9290\n",
      "Epoch 93, time 81.5079, train_loss: 0.6836, test_loss: 0.9431\n",
      "Epoch 94, time 82.3780, train_loss: 0.6809, test_loss: 0.9342\n",
      "Epoch 95, time 83.2464, train_loss: 0.6928, test_loss: 0.9141\n",
      "Epoch 96, time 84.1194, train_loss: 0.6855, test_loss: 0.9385\n",
      "Epoch 97, time 84.9920, train_loss: 0.6720, test_loss: 0.9802\n",
      "Epoch 98, time 85.8650, train_loss: 0.6730, test_loss: 0.9605\n",
      "Epoch 99, time 86.7366, train_loss: 0.6708, test_loss: 0.9621\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8uBSgqTxXcj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWLI02VPxXcj"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4483    | 0.7327    |\n",
    "| GT without edge features (only atom type)    | 0.6583   | 0.9095    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d26mlnkvxXcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j73ydTFQxXcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
