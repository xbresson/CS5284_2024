{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfFWcbbExXcc"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra) -- Solution\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1730637248755,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "IZCvd1fTxXce",
    "outputId": "41ceed4f-96e9-4b1a-a395-f72c3621d79d"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y9hiy25BxXcf"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Es6_zDxXcf"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1730637253097,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fl68-dJTxXcf",
    "outputId": "7f142b87-0e09-4434-d76b-18d370ec5f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 0.9511s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types\n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTYjHxtUxXcg"
   },
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1730637255093,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "TQp3RdpAxXcg",
    "outputId": "19126433-8031-4aa7-d982-1cd2d516a142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node positional encoding features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature\n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrFXYNexXcg"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1730637255907,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "i77JNkdBxXcg",
    "outputId": "a2e5b1cf-7e62-4fe9-8653-d1fb55bab668"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R14LMFIRxXch"
   },
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "d7ZXaxwKxXch",
    "outputId": "af206596-e7cd-41ce-9b51-20b2b9e4167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=87, num_edges=180,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[-0.5696],\n",
      "        [ 0.0930],\n",
      "        [-0.0904],\n",
      "        [-1.5065],\n",
      "        [-0.4760],\n",
      "        [ 0.1606],\n",
      "        [-0.2795],\n",
      "        [ 2.8563],\n",
      "        [-1.0082],\n",
      "        [-2.6547]])\n",
      "batch_x: torch.Size([87])\n",
      "batch_pe: torch.Size([87, 3])\n",
      "batch_e: torch.Size([180])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBKKjxWxXch"
   },
   "source": [
    "# Exercise 1: Design the class of GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqI7VXd9gchf"
   },
   "source": [
    "### Question 1.1: Implement a Graph Multi-Head Attention (MHA) Layer with edge features\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute bi-linear products with edge feature: $q_i^T * diag(e_{ij}) * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j, edges.data[]``` form ```ij```\".  \n",
    "\n",
    "    - *Step 1.2* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V```.\n",
    "\n",
    "    - *Step 1.4:* Compute edge feature: $q_i^T * diag(e_{ij}) * k_j$.\n",
    "\n",
    "    - *Step 1.5:* Update edge feature.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V, E, F, G```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V', 'E', 'F', 'G'``` for ```g.ndata[]``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XQ-OgcDJ7rC4"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute bi-linear products with edge feature\n",
    "        qikj = (edges.src['K'] * edges.data['E'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "\n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) )\n",
    "\n",
    "        # Step 1.3: Obtain vj\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "\n",
    "        # Step 1.4: Compute edge feature: e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        eij = edges.src['K'] * edges.data['E'] * edges.dst['Q'] / torch.sqrt(torch.tensor(self.head_hidden_dim))\n",
    "\n",
    "        # Step 1.5: Update edge feature\n",
    "        edges.data['e'] = eij\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij = nodes.mailbox['expij']\n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj = nodes.mailbox['vj']\n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 )\n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator = torch.sum( expij, dim=1 )\n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJix5TqbrFhR"
   },
   "source": [
    "### Question 1.2: Implement a Graph Transformer layer (with edge feature)\n",
    "\n",
    "- Implement dropout, layer normalization, and residual connection layers for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Roe4rKU371mH"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Dropout layers for edge features\n",
    "        self.dropout_e_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout) # dropout value\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "\n",
    "        # Layer normalization for edge features\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # layer normalization for edge features, size=(N, d)\n",
    "        e = self.layer_norm1e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Update for edge features\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h, e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rST8IpB2rYqK"
   },
   "source": [
    "### Question 1.3: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network (with edge feature)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Adding a input edge embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer (with edge feature):* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2DQRl2bYxXch"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Initialize a edge embedding layer\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe, e):\n",
    "\n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement teh edge embedding layer\n",
    "        # size=(num_edges, hidden_dim)\n",
    "        e = self.embedding_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qzWVgqKW7vSU",
    "outputId": "3703d3bb-d48e-43d6-ed06-8723e11cb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deeplearn/miniconda3/envs/gnn_course_gpu/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0gZzixzxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291348,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Hw7LEG5exXci",
    "outputId": "ead6b872-7372-4f0c-cffa-845c8147baff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 667137 (0.67 million)\n",
      "Epoch 0, time 1.2384, train_loss: 1.3021, test_loss: 1.1779\n",
      "Epoch 1, time 2.4472, train_loss: 1.0910, test_loss: 1.0079\n",
      "Epoch 2, time 3.6719, train_loss: 1.0110, test_loss: 1.0140\n",
      "Epoch 3, time 4.9203, train_loss: 1.0068, test_loss: 0.9544\n",
      "Epoch 4, time 6.1670, train_loss: 0.9861, test_loss: 0.9758\n",
      "Epoch 5, time 7.4124, train_loss: 0.9583, test_loss: 0.9269\n",
      "Epoch 6, time 8.6694, train_loss: 0.9301, test_loss: 0.9861\n",
      "Epoch 7, time 9.9118, train_loss: 0.9210, test_loss: 0.8910\n",
      "Epoch 8, time 11.1505, train_loss: 0.9125, test_loss: 0.9205\n",
      "Epoch 9, time 12.4073, train_loss: 0.8883, test_loss: 0.9069\n",
      "Epoch 10, time 13.6029, train_loss: 0.8928, test_loss: 0.9191\n",
      "Epoch 11, time 14.8157, train_loss: 0.8676, test_loss: 0.9459\n",
      "Epoch 12, time 16.0094, train_loss: 0.8506, test_loss: 0.8151\n",
      "Epoch 13, time 17.2371, train_loss: 0.8327, test_loss: 0.8209\n",
      "Epoch 14, time 18.4189, train_loss: 0.8465, test_loss: 0.8554\n",
      "Epoch 15, time 19.5999, train_loss: 0.8133, test_loss: 0.8707\n",
      "Epoch 16, time 20.7904, train_loss: 0.8161, test_loss: 0.8505\n",
      "Epoch 17, time 21.9973, train_loss: 0.8093, test_loss: 0.7911\n",
      "Epoch 18, time 23.1802, train_loss: 0.7872, test_loss: 0.8218\n",
      "Epoch 19, time 24.3717, train_loss: 0.8324, test_loss: 0.8727\n",
      "Epoch 20, time 25.5646, train_loss: 0.7853, test_loss: 0.8112\n",
      "Epoch 21, time 26.7582, train_loss: 0.7590, test_loss: 0.7792\n",
      "Epoch 22, time 27.8825, train_loss: 0.7539, test_loss: 0.7835\n",
      "Epoch 23, time 29.0839, train_loss: 0.7419, test_loss: 0.8258\n",
      "Epoch 24, time 30.2944, train_loss: 0.7501, test_loss: 0.7658\n",
      "Epoch 25, time 31.4916, train_loss: 0.7428, test_loss: 0.7639\n",
      "Epoch 26, time 32.6307, train_loss: 0.7163, test_loss: 0.7456\n",
      "Epoch 27, time 33.7642, train_loss: 0.7452, test_loss: 0.8335\n",
      "Epoch 28, time 34.9023, train_loss: 0.7357, test_loss: 0.8847\n",
      "Epoch 29, time 36.0212, train_loss: 0.7399, test_loss: 0.7799\n",
      "Epoch 30, time 37.1199, train_loss: 0.6921, test_loss: 0.7514\n",
      "Epoch 31, time 38.2405, train_loss: 0.6892, test_loss: 0.7381\n",
      "Epoch 32, time 39.3934, train_loss: 0.6972, test_loss: 0.7578\n",
      "Epoch 33, time 40.5426, train_loss: 0.6771, test_loss: 0.7989\n",
      "Epoch 34, time 41.6948, train_loss: 0.6641, test_loss: 0.7879\n",
      "Epoch 35, time 42.8709, train_loss: 0.6990, test_loss: 0.7694\n",
      "Epoch 36, time 44.0294, train_loss: 0.6616, test_loss: 0.7379\n",
      "Epoch 37, time 45.1902, train_loss: 0.6674, test_loss: 0.7219\n",
      "Epoch 38, time 46.3520, train_loss: 0.6547, test_loss: 0.7056\n",
      "Epoch 39, time 47.5147, train_loss: 0.6445, test_loss: 0.7303\n",
      "Epoch 40, time 48.6677, train_loss: 0.6270, test_loss: 0.7221\n",
      "Epoch 41, time 49.7977, train_loss: 0.6418, test_loss: 0.7091\n",
      "Epoch 42, time 50.9199, train_loss: 0.6427, test_loss: 0.7206\n",
      "Epoch 43, time 52.0635, train_loss: 0.6240, test_loss: 0.7009\n",
      "Epoch 44, time 53.2170, train_loss: 0.6107, test_loss: 0.7315\n",
      "Epoch 45, time 54.3481, train_loss: 0.6154, test_loss: 0.7568\n",
      "Epoch 46, time 55.4711, train_loss: 0.6033, test_loss: 0.7091\n",
      "Epoch 47, time 56.6117, train_loss: 0.6149, test_loss: 0.7188\n",
      "Epoch 48, time 57.7397, train_loss: 0.6117, test_loss: 0.7025\n",
      "Epoch 49, time 58.8813, train_loss: 0.5931, test_loss: 0.7543\n",
      "Epoch 50, time 60.0448, train_loss: 0.5782, test_loss: 0.7182\n",
      "Epoch 51, time 61.2201, train_loss: 0.5941, test_loss: 0.7393\n",
      "Epoch 52, time 62.4000, train_loss: 0.5901, test_loss: 0.7541\n",
      "Epoch 53, time 63.5832, train_loss: 0.5987, test_loss: 0.7185\n",
      "Epoch 54, time 64.7538, train_loss: 0.5801, test_loss: 0.7238\n",
      "Epoch 55, time 65.9281, train_loss: 0.5662, test_loss: 0.7409\n",
      "Epoch 56, time 67.1088, train_loss: 0.5760, test_loss: 0.7377\n",
      "Epoch 57, time 68.2963, train_loss: 0.5686, test_loss: 0.7188\n",
      "Epoch 58, time 69.4665, train_loss: 0.5983, test_loss: 0.7208\n",
      "Epoch 59, time 70.6501, train_loss: 0.6038, test_loss: 0.7501\n",
      "Epoch 60, time 71.8250, train_loss: 0.5771, test_loss: 0.7210\n",
      "Epoch 61, time 73.0069, train_loss: 0.5612, test_loss: 0.7245\n",
      "Epoch 62, time 74.1743, train_loss: 0.5385, test_loss: 0.7380\n",
      "Epoch 63, time 75.3244, train_loss: 0.5320, test_loss: 0.7339\n",
      "Epoch 64, time 76.4075, train_loss: 0.5348, test_loss: 0.7164\n",
      "Epoch 65, time 77.5254, train_loss: 0.5528, test_loss: 0.7261\n",
      "Epoch 66, time 78.6541, train_loss: 0.5397, test_loss: 0.7323\n",
      "Epoch 67, time 79.7791, train_loss: 0.5446, test_loss: 0.7427\n",
      "Epoch 68, time 80.9200, train_loss: 0.5365, test_loss: 0.7624\n",
      "Epoch 69, time 82.0665, train_loss: 0.5444, test_loss: 0.7191\n",
      "Epoch 70, time 83.2040, train_loss: 0.5459, test_loss: 0.7491\n",
      "Epoch 71, time 84.3420, train_loss: 0.5507, test_loss: 0.7428\n",
      "Epoch 72, time 85.4884, train_loss: 0.5372, test_loss: 0.7464\n",
      "Epoch 73, time 86.6245, train_loss: 0.5232, test_loss: 0.7452\n",
      "Epoch 74, time 87.7485, train_loss: 0.5176, test_loss: 0.7696\n",
      "Epoch 75, time 88.8809, train_loss: 0.5120, test_loss: 0.7465\n",
      "Epoch 76, time 90.0649, train_loss: 0.5009, test_loss: 0.7459\n",
      "Epoch 77, time 91.3026, train_loss: 0.5209, test_loss: 0.7355\n",
      "Epoch 78, time 92.4244, train_loss: 0.5356, test_loss: 0.7464\n",
      "Epoch 79, time 93.5993, train_loss: 0.5112, test_loss: 0.7140\n",
      "Epoch 80, time 94.7224, train_loss: 0.4932, test_loss: 0.7214\n",
      "Epoch 81, time 95.8483, train_loss: 0.4986, test_loss: 0.7168\n",
      "Epoch 82, time 96.9789, train_loss: 0.5108, test_loss: 0.7415\n",
      "Epoch 83, time 98.1140, train_loss: 0.4887, test_loss: 0.7404\n",
      "Epoch 84, time 99.2471, train_loss: 0.4890, test_loss: 0.7618\n",
      "Epoch 85, time 100.3846, train_loss: 0.4983, test_loss: 0.7378\n",
      "Epoch 86, time 101.5010, train_loss: 0.5128, test_loss: 0.7671\n",
      "Epoch 87, time 102.6770, train_loss: 0.5079, test_loss: 0.7437\n",
      "Epoch 88, time 103.8906, train_loss: 0.4786, test_loss: 0.7362\n",
      "Epoch 89, time 105.0066, train_loss: 0.4863, test_loss: 0.7263\n",
      "Epoch 90, time 106.1360, train_loss: 0.4759, test_loss: 0.7185\n",
      "Epoch 91, time 107.2557, train_loss: 0.4807, test_loss: 0.7344\n",
      "Epoch 92, time 108.3600, train_loss: 0.4761, test_loss: 0.7302\n",
      "Epoch 93, time 109.4851, train_loss: 0.4665, test_loss: 0.7358\n",
      "Epoch 94, time 110.6227, train_loss: 0.4516, test_loss: 0.7425\n",
      "Epoch 95, time 111.7499, train_loss: 0.4664, test_loss: 0.7289\n",
      "Epoch 96, time 112.8663, train_loss: 0.4667, test_loss: 0.7549\n",
      "Epoch 97, time 113.9770, train_loss: 0.4794, test_loss: 0.7289\n",
      "Epoch 98, time 115.2034, train_loss: 0.4676, test_loss: 0.7488\n",
      "Epoch 99, time 116.3394, train_loss: 0.4716, test_loss: 0.7281\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCjHW1YxXci"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amo_4YiKxXci"
   },
   "source": [
    "# GT without edge features\n",
    "\n",
    "Node update equation:\n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BKceFO_W8HBf"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        # Compute the dot products q_i^T * k_j\n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\"\n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lWOIc6W_8JY4"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_Gaf_Vi8MQt"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe):\n",
    "\n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "rPtk6EjKxXci",
    "outputId": "8db1c7ef-bbc3-4f67-df10-3ce74c4fac93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jt8BzHJxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168398,
     "status": "ok",
     "timestamp": 1730637716334,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xAdlIhxXxXci",
    "outputId": "ec39dc6a-0677-4b9e-93a0-d79426d21d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 0.8411, train_loss: 1.3158, test_loss: 1.2534\n",
      "Epoch 1, time 1.7289, train_loss: 1.2473, test_loss: 1.2224\n",
      "Epoch 2, time 2.6348, train_loss: 1.1546, test_loss: 1.1590\n",
      "Epoch 3, time 3.5190, train_loss: 1.1281, test_loss: 1.1592\n",
      "Epoch 4, time 4.3971, train_loss: 1.1152, test_loss: 1.1192\n",
      "Epoch 5, time 5.2760, train_loss: 1.0980, test_loss: 1.1247\n",
      "Epoch 6, time 6.1606, train_loss: 1.0543, test_loss: 1.0459\n",
      "Epoch 7, time 7.0451, train_loss: 1.0686, test_loss: 1.0720\n",
      "Epoch 8, time 7.9234, train_loss: 1.0478, test_loss: 1.0160\n",
      "Epoch 9, time 8.8173, train_loss: 0.9971, test_loss: 0.9909\n",
      "Epoch 10, time 9.7070, train_loss: 1.0056, test_loss: 0.9907\n",
      "Epoch 11, time 10.5708, train_loss: 0.9649, test_loss: 0.9502\n",
      "Epoch 12, time 11.5574, train_loss: 0.9478, test_loss: 0.9553\n",
      "Epoch 13, time 12.4050, train_loss: 0.9643, test_loss: 1.0673\n",
      "Epoch 14, time 13.2820, train_loss: 0.9720, test_loss: 0.9509\n",
      "Epoch 15, time 14.1725, train_loss: 0.9300, test_loss: 0.9814\n",
      "Epoch 16, time 15.0376, train_loss: 0.9565, test_loss: 0.9642\n",
      "Epoch 17, time 15.9360, train_loss: 0.9183, test_loss: 0.9250\n",
      "Epoch 18, time 16.8673, train_loss: 0.9145, test_loss: 0.9077\n",
      "Epoch 19, time 17.7874, train_loss: 0.9331, test_loss: 0.9705\n",
      "Epoch 20, time 18.6897, train_loss: 0.9322, test_loss: 0.9260\n",
      "Epoch 21, time 19.5861, train_loss: 0.8866, test_loss: 0.8934\n",
      "Epoch 22, time 20.4798, train_loss: 0.9247, test_loss: 0.9866\n",
      "Epoch 23, time 21.3799, train_loss: 0.9518, test_loss: 0.9858\n",
      "Epoch 24, time 22.2927, train_loss: 0.9406, test_loss: 0.9571\n",
      "Epoch 25, time 23.1989, train_loss: 0.9092, test_loss: 0.9160\n",
      "Epoch 26, time 24.0991, train_loss: 0.9106, test_loss: 0.9213\n",
      "Epoch 27, time 24.9933, train_loss: 0.8806, test_loss: 0.9793\n",
      "Epoch 28, time 25.9757, train_loss: 0.8937, test_loss: 0.9008\n",
      "Epoch 29, time 26.9255, train_loss: 0.8828, test_loss: 0.8870\n",
      "Epoch 30, time 27.8532, train_loss: 0.8789, test_loss: 0.9583\n",
      "Epoch 31, time 28.7296, train_loss: 0.8692, test_loss: 0.9124\n",
      "Epoch 32, time 29.6025, train_loss: 0.8799, test_loss: 0.9760\n",
      "Epoch 33, time 30.5569, train_loss: 0.8929, test_loss: 0.8900\n",
      "Epoch 34, time 31.4372, train_loss: 0.8853, test_loss: 0.8991\n",
      "Epoch 35, time 32.2686, train_loss: 0.8762, test_loss: 0.9172\n",
      "Epoch 36, time 33.1405, train_loss: 0.8774, test_loss: 0.9020\n",
      "Epoch 37, time 34.0763, train_loss: 0.8726, test_loss: 0.8987\n",
      "Epoch 38, time 35.0212, train_loss: 0.8448, test_loss: 0.9114\n",
      "Epoch 39, time 35.8893, train_loss: 0.8553, test_loss: 0.8921\n",
      "Epoch 40, time 36.7587, train_loss: 0.8533, test_loss: 0.8863\n",
      "Epoch 41, time 37.6280, train_loss: 0.8516, test_loss: 0.9076\n",
      "Epoch 42, time 38.4964, train_loss: 0.8632, test_loss: 0.9216\n",
      "Epoch 43, time 39.3709, train_loss: 0.8563, test_loss: 0.9015\n",
      "Epoch 44, time 40.2400, train_loss: 0.8376, test_loss: 0.9167\n",
      "Epoch 45, time 41.1080, train_loss: 0.8450, test_loss: 0.9469\n",
      "Epoch 46, time 41.9779, train_loss: 0.8353, test_loss: 0.8908\n",
      "Epoch 47, time 42.8468, train_loss: 0.8172, test_loss: 0.8892\n",
      "Epoch 48, time 43.7182, train_loss: 0.8362, test_loss: 0.8673\n",
      "Epoch 49, time 44.6509, train_loss: 0.8362, test_loss: 0.8854\n",
      "Epoch 50, time 45.5666, train_loss: 0.8391, test_loss: 0.8652\n",
      "Epoch 51, time 46.4767, train_loss: 0.8409, test_loss: 0.9640\n",
      "Epoch 52, time 47.4006, train_loss: 0.8307, test_loss: 0.8915\n",
      "Epoch 53, time 48.3225, train_loss: 0.8390, test_loss: 0.9014\n",
      "Epoch 54, time 49.2060, train_loss: 0.8397, test_loss: 0.8717\n",
      "Epoch 55, time 50.0724, train_loss: 0.8415, test_loss: 0.9040\n",
      "Epoch 56, time 50.9398, train_loss: 0.8482, test_loss: 0.8818\n",
      "Epoch 57, time 51.7689, train_loss: 0.8370, test_loss: 0.9177\n",
      "Epoch 58, time 52.5973, train_loss: 0.8093, test_loss: 0.8868\n",
      "Epoch 59, time 53.4290, train_loss: 0.7970, test_loss: 0.9014\n",
      "Epoch 60, time 54.2915, train_loss: 0.8066, test_loss: 0.8900\n",
      "Epoch 61, time 55.1659, train_loss: 0.8166, test_loss: 0.9043\n",
      "Epoch 62, time 56.0373, train_loss: 0.8140, test_loss: 0.8792\n",
      "Epoch 63, time 56.9101, train_loss: 0.7941, test_loss: 0.9084\n",
      "Epoch 64, time 57.7806, train_loss: 0.7912, test_loss: 0.8814\n",
      "Epoch 65, time 58.6461, train_loss: 0.7878, test_loss: 0.9120\n",
      "Epoch 66, time 59.5556, train_loss: 0.7911, test_loss: 0.8969\n",
      "Epoch 67, time 60.4334, train_loss: 0.7671, test_loss: 0.9102\n",
      "Epoch 68, time 61.2840, train_loss: 0.7948, test_loss: 0.8760\n",
      "Epoch 69, time 62.1400, train_loss: 0.7856, test_loss: 0.8705\n",
      "Epoch 70, time 63.0139, train_loss: 0.7699, test_loss: 0.8991\n",
      "Epoch 71, time 63.9486, train_loss: 0.7673, test_loss: 0.9389\n",
      "Epoch 72, time 64.8138, train_loss: 0.7703, test_loss: 0.8861\n",
      "Epoch 73, time 65.6866, train_loss: 0.7708, test_loss: 0.8857\n",
      "Epoch 74, time 66.5447, train_loss: 0.7608, test_loss: 0.9062\n",
      "Epoch 75, time 67.4021, train_loss: 0.7612, test_loss: 0.8673\n",
      "Epoch 76, time 68.2843, train_loss: 0.7457, test_loss: 0.9067\n",
      "Epoch 77, time 69.1119, train_loss: 0.7515, test_loss: 0.8754\n",
      "Epoch 78, time 69.9710, train_loss: 0.7323, test_loss: 0.8651\n",
      "Epoch 79, time 70.8437, train_loss: 0.7656, test_loss: 0.9076\n",
      "Epoch 80, time 71.7232, train_loss: 0.7387, test_loss: 0.9140\n",
      "Epoch 81, time 72.6024, train_loss: 0.7417, test_loss: 0.8941\n",
      "Epoch 82, time 73.5481, train_loss: 0.7170, test_loss: 0.8811\n",
      "Epoch 83, time 74.4181, train_loss: 0.7605, test_loss: 0.9109\n",
      "Epoch 84, time 75.3140, train_loss: 0.7408, test_loss: 0.8829\n",
      "Epoch 85, time 76.1941, train_loss: 0.7306, test_loss: 0.8871\n",
      "Epoch 86, time 77.1730, train_loss: 0.7275, test_loss: 0.8842\n",
      "Epoch 87, time 78.0468, train_loss: 0.7335, test_loss: 0.9039\n",
      "Epoch 88, time 78.9202, train_loss: 0.7029, test_loss: 0.8807\n",
      "Epoch 89, time 79.7887, train_loss: 0.7070, test_loss: 0.9108\n",
      "Epoch 90, time 80.6596, train_loss: 0.7150, test_loss: 0.9321\n",
      "Epoch 91, time 81.5293, train_loss: 0.7139, test_loss: 0.9396\n",
      "Epoch 92, time 82.3880, train_loss: 0.7007, test_loss: 0.9178\n",
      "Epoch 93, time 83.2467, train_loss: 0.7443, test_loss: 0.9701\n",
      "Epoch 94, time 84.0950, train_loss: 0.7197, test_loss: 0.9005\n",
      "Epoch 95, time 84.9301, train_loss: 0.6969, test_loss: 0.9464\n",
      "Epoch 96, time 85.7654, train_loss: 0.7009, test_loss: 0.9341\n",
      "Epoch 97, time 86.6017, train_loss: 0.6999, test_loss: 0.8794\n",
      "Epoch 98, time 87.5095, train_loss: 0.6815, test_loss: 0.8954\n",
      "Epoch 99, time 88.3573, train_loss: 0.6715, test_loss: 0.9001\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8uBSgqTxXcj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWLI02VPxXcj"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4483    | 0.7327    |\n",
    "| GT without edge features (only atom type)    | 0.6583   | 0.9095    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d26mlnkvxXcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j73ydTFQxXcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
