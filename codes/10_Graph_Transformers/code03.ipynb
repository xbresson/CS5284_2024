{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra)\n",
    "\n",
    "### Xavier Bresson\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 0.5021s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types \n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node and edge features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature \n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=90, num_edges=192,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 3.9472],\n",
      "        [-4.0181],\n",
      "        [ 1.9524],\n",
      "        [-1.5489],\n",
      "        [ 0.5479],\n",
      "        [-1.0082],\n",
      "        [ 3.5653],\n",
      "        [-1.1304],\n",
      "        [ 2.6200],\n",
      "        [-7.4328]])\n",
      "batch_x: torch.Size([90])\n",
      "batch_pe: torch.Size([90, 3])\n",
      "batch_e: torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the class of GraphTransformer networks with edge features \n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WF): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WG): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbresson/miniconda3/envs/gnn_course/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# class graph multi head attention layer  \n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "    \n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WF = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WG = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges): \n",
    "        # Compute bi-linear products with edge feature : q_i^T * diag(e_ij) * k_j \n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j, edges.data[] form ij\" \n",
    "        qikj = (edges.src['K'] * edges.data['E'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "        #qikj = (edges.src['K'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # NO edge feature\n",
    "        #qikj = ### YOUR CODE HERE, NO edge feature\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        # Compute edge feature : q_i^T * diag(e_ij) * k_j\n",
    "        eij = edges.src['K'] * edges.data['E'] * edges.dst['Q'] / torch.sqrt(torch.tensor(self.head_hidden_dim)) # e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        #eij = ### YOUR CODE HERE, size=(E,K,d')\n",
    "        edges.data['e'] = eij # update edge feature \n",
    "        return {'expij' : expij, 'vj' : vj} \n",
    "    \n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h} \n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "        F = self.WF(h) # size=(N, d)\n",
    "        G = self.WG(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.ndata['F'] = F.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['G'] = G.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA \n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "    \n",
    "    \n",
    "# class GraphTransformer layer  \n",
    "class GraphTransformer_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        \n",
    "    def forward(self, g, h, e): \n",
    "        \n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm1e(e) # layer normalization, size=(N, d)\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    \n",
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ]) \n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)  \n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        \n",
    "    def forward(self, g, h, pe, e):\n",
    "        \n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # if PE used\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # input edge embedding\n",
    "        e = self.embedding_e(e) # size=(num_edges, hidden_dim)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)  \n",
    "        y = self.ln_h_final(mol_token)  \n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)    \n",
    "\n",
    "        return y    \n",
    "    \n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 799233 (0.80 million)\n",
      "Epoch 0, time 3.4870, train_loss: 1.3605, test_loss: 1.2832\n",
      "Epoch 1, time 6.6361, train_loss: 1.1895, test_loss: 1.0722\n",
      "Epoch 2, time 8.8625, train_loss: 1.0280, test_loss: 0.9831\n",
      "Epoch 3, time 11.1587, train_loss: 1.0266, test_loss: 0.9721\n",
      "Epoch 4, time 15.6749, train_loss: 0.9678, test_loss: 0.9966\n",
      "Epoch 5, time 18.2747, train_loss: 0.9638, test_loss: 0.9225\n",
      "Epoch 6, time 21.0403, train_loss: 0.9349, test_loss: 0.9462\n",
      "Epoch 7, time 23.7075, train_loss: 0.9111, test_loss: 0.8912\n",
      "Epoch 8, time 29.4418, train_loss: 0.9031, test_loss: 0.9032\n",
      "Epoch 9, time 37.6699, train_loss: 0.8862, test_loss: 0.9204\n",
      "Epoch 10, time 43.6222, train_loss: 0.9059, test_loss: 0.8840\n",
      "Epoch 11, time 47.5393, train_loss: 0.8541, test_loss: 0.8649\n",
      "Epoch 12, time 51.4519, train_loss: 0.8321, test_loss: 0.8568\n",
      "Epoch 13, time 54.2070, train_loss: 0.8430, test_loss: 0.8646\n",
      "Epoch 14, time 58.3196, train_loss: 0.8201, test_loss: 0.8110\n",
      "Epoch 15, time 64.0831, train_loss: 0.8194, test_loss: 0.8127\n",
      "Epoch 16, time 67.1834, train_loss: 0.7959, test_loss: 0.8475\n",
      "Epoch 17, time 69.4214, train_loss: 0.8282, test_loss: 0.8350\n",
      "Epoch 18, time 71.8728, train_loss: 0.7968, test_loss: 0.7991\n",
      "Epoch 19, time 74.0111, train_loss: 0.7669, test_loss: 0.7981\n",
      "Epoch 20, time 76.1445, train_loss: 0.7729, test_loss: 0.8089\n",
      "Epoch 21, time 78.0059, train_loss: 0.7597, test_loss: 0.8390\n",
      "Epoch 22, time 80.0414, train_loss: 0.7753, test_loss: 0.8534\n",
      "Epoch 23, time 82.0421, train_loss: 0.7779, test_loss: 0.7900\n",
      "Epoch 24, time 84.1979, train_loss: 0.7365, test_loss: 0.8297\n",
      "Epoch 25, time 86.6382, train_loss: 0.7298, test_loss: 0.8318\n",
      "Epoch 26, time 92.4859, train_loss: 0.7270, test_loss: 0.8038\n",
      "Epoch 27, time 97.7286, train_loss: 0.7179, test_loss: 0.7818\n",
      "Epoch 28, time 100.2008, train_loss: 0.7033, test_loss: 0.7712\n",
      "Epoch 29, time 102.7784, train_loss: 0.7147, test_loss: 0.8802\n",
      "Epoch 30, time 105.1027, train_loss: 0.7753, test_loss: 0.7967\n",
      "Epoch 31, time 107.5590, train_loss: 0.7224, test_loss: 0.8136\n",
      "Epoch 32, time 111.7369, train_loss: 0.7414, test_loss: 0.7978\n",
      "Epoch 33, time 119.3706, train_loss: 0.7131, test_loss: 0.7960\n",
      "Epoch 34, time 122.0474, train_loss: 0.6762, test_loss: 0.7683\n",
      "Epoch 35, time 124.1017, train_loss: 0.6679, test_loss: 0.7672\n",
      "Epoch 36, time 127.5589, train_loss: 0.6578, test_loss: 0.7738\n",
      "Epoch 37, time 130.6876, train_loss: 0.6942, test_loss: 0.8245\n",
      "Epoch 38, time 132.9661, train_loss: 0.6669, test_loss: 0.7573\n",
      "Epoch 39, time 135.0046, train_loss: 0.6751, test_loss: 0.7913\n",
      "Epoch 40, time 137.2794, train_loss: 0.6458, test_loss: 0.7541\n",
      "Epoch 41, time 139.3023, train_loss: 0.6513, test_loss: 0.7633\n",
      "Epoch 42, time 141.2717, train_loss: 0.6406, test_loss: 0.7716\n",
      "Epoch 43, time 143.2270, train_loss: 0.6231, test_loss: 0.7632\n",
      "Epoch 44, time 145.8055, train_loss: 0.6173, test_loss: 0.7647\n",
      "Epoch 45, time 150.7853, train_loss: 0.6247, test_loss: 0.7679\n",
      "Epoch 46, time 154.4987, train_loss: 0.6036, test_loss: 0.7332\n",
      "Epoch 47, time 160.1497, train_loss: 0.6127, test_loss: 0.7880\n",
      "Epoch 48, time 163.5426, train_loss: 0.6222, test_loss: 0.7750\n",
      "Epoch 49, time 167.4833, train_loss: 0.6116, test_loss: 0.7472\n",
      "Epoch 50, time 171.1464, train_loss: 0.5974, test_loss: 0.7648\n",
      "Epoch 51, time 174.9051, train_loss: 0.5803, test_loss: 0.7292\n",
      "Epoch 52, time 183.2566, train_loss: 0.5795, test_loss: 0.7381\n",
      "Epoch 53, time 189.8585, train_loss: 0.5675, test_loss: 0.7767\n",
      "Epoch 54, time 195.2193, train_loss: 0.5915, test_loss: 0.8069\n",
      "Epoch 55, time 198.1970, train_loss: 0.5979, test_loss: 0.7461\n",
      "Epoch 56, time 201.4087, train_loss: 0.5628, test_loss: 0.7547\n",
      "Epoch 57, time 204.0171, train_loss: 0.5664, test_loss: 0.7510\n",
      "Epoch 58, time 206.4138, train_loss: 0.5646, test_loss: 0.7540\n",
      "Epoch 59, time 210.3636, train_loss: 0.5597, test_loss: 0.7413\n",
      "Epoch 60, time 212.6730, train_loss: 0.5600, test_loss: 0.7381\n",
      "Epoch 61, time 216.4772, train_loss: 0.5330, test_loss: 0.7375\n",
      "Epoch 62, time 219.0107, train_loss: 0.5447, test_loss: 0.7416\n",
      "Epoch 63, time 222.4475, train_loss: 0.5476, test_loss: 0.7599\n",
      "Epoch 64, time 225.3777, train_loss: 0.5695, test_loss: 0.7525\n",
      "Epoch 65, time 227.7391, train_loss: 0.5404, test_loss: 0.7289\n",
      "Epoch 66, time 230.2811, train_loss: 0.5468, test_loss: 0.7729\n",
      "Epoch 67, time 232.5687, train_loss: 0.5512, test_loss: 0.7727\n",
      "Epoch 68, time 234.7820, train_loss: 0.5482, test_loss: 0.7588\n",
      "Epoch 69, time 238.0668, train_loss: 0.5291, test_loss: 0.7437\n",
      "Epoch 70, time 240.4660, train_loss: 0.5285, test_loss: 0.7548\n",
      "Epoch 71, time 242.6238, train_loss: 0.5237, test_loss: 0.7411\n",
      "Epoch 72, time 244.9372, train_loss: 0.5240, test_loss: 0.7389\n",
      "Epoch 73, time 247.4939, train_loss: 0.5130, test_loss: 0.7448\n",
      "Epoch 74, time 250.6976, train_loss: 0.5112, test_loss: 0.7297\n",
      "Epoch 75, time 258.0838, train_loss: 0.5117, test_loss: 0.7527\n",
      "Epoch 76, time 260.4471, train_loss: 0.5135, test_loss: 0.7228\n",
      "Epoch 77, time 264.3439, train_loss: 0.5315, test_loss: 0.7443\n",
      "Epoch 78, time 266.4445, train_loss: 0.5361, test_loss: 0.7450\n",
      "Epoch 79, time 269.8797, train_loss: 0.5305, test_loss: 0.7738\n",
      "Epoch 80, time 272.1050, train_loss: 0.4930, test_loss: 0.7334\n",
      "Epoch 81, time 275.1155, train_loss: 0.4878, test_loss: 0.7706\n",
      "Epoch 82, time 277.5396, train_loss: 0.4800, test_loss: 0.7557\n",
      "Epoch 83, time 279.8515, train_loss: 0.4718, test_loss: 0.7428\n",
      "Epoch 84, time 281.9546, train_loss: 0.4789, test_loss: 0.7682\n",
      "Epoch 85, time 290.4074, train_loss: 0.5017, test_loss: 0.7627\n",
      "Epoch 86, time 292.5606, train_loss: 0.5060, test_loss: 0.7530\n",
      "Epoch 87, time 296.3100, train_loss: 0.4924, test_loss: 0.7508\n",
      "Epoch 88, time 300.1837, train_loss: 0.4764, test_loss: 0.7605\n",
      "Epoch 89, time 302.5290, train_loss: 0.4666, test_loss: 0.7473\n",
      "Epoch 90, time 305.0404, train_loss: 0.4597, test_loss: 0.7561\n",
      "Epoch 91, time 307.3914, train_loss: 0.4623, test_loss: 0.7568\n",
      "Epoch 92, time 309.5606, train_loss: 0.4458, test_loss: 0.7566\n",
      "Epoch 93, time 311.6123, train_loss: 0.4571, test_loss: 0.7727\n",
      "Epoch 94, time 313.7558, train_loss: 0.4624, test_loss: 0.7902\n",
      "Epoch 95, time 317.0031, train_loss: 0.4629, test_loss: 0.7761\n",
      "Epoch 96, time 319.2548, train_loss: 0.4448, test_loss: 0.7428\n",
      "Epoch 97, time 321.2933, train_loss: 0.4714, test_loss: 0.7641\n",
      "Epoch 98, time 323.4524, train_loss: 0.4584, test_loss: 0.7607\n",
      "Epoch 99, time 325.6435, train_loss: 0.4335, test_loss: 0.7967\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT without edge features \n",
    "\n",
    "Node update equation: \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# class graph multi head attention layer  \n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "    \n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges): \n",
    "        # Compute the dot products q_i^T * k_j \n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\" \n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), , edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj} \n",
    "    \n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        #numerator = ### YOUR CODE HERE, sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        #denominator = ### YOUR CODE HERE, sum_j' exp_ij', size=(N,K,1)\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h} \n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA \n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "    \n",
    "    \n",
    "# class GraphTransformer layer  \n",
    "class GraphTransformer_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        \n",
    "    def forward(self, g, h): \n",
    "        \n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ]) \n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)  \n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        \n",
    "    def forward(self, g, h, pe):\n",
    "        \n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # input node embedding = positional embedding\n",
    "        # Compute the linear transformation of the positional embedding \n",
    "        # You may use \"nn.Linear(pos_enc_dim, hidden_dim)\"\n",
    "        #h = ### YOUR CODE HERE, Lap eigenvectors fecture, size=(num_nodes, hidden_dim)\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)  \n",
    "        y = self.ln_h_final(mol_token)  \n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)    \n",
    "\n",
    "        return y    \n",
    "\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 2.2989, train_loss: 1.4045, test_loss: 1.3181\n",
      "Epoch 1, time 4.0101, train_loss: 1.2654, test_loss: 1.2426\n",
      "Epoch 2, time 5.2809, train_loss: 1.1938, test_loss: 1.2187\n",
      "Epoch 3, time 6.5593, train_loss: 1.1660, test_loss: 1.1739\n",
      "Epoch 4, time 8.4813, train_loss: 1.1377, test_loss: 1.1733\n",
      "Epoch 5, time 10.2418, train_loss: 1.0971, test_loss: 1.1252\n",
      "Epoch 6, time 11.4873, train_loss: 1.0914, test_loss: 1.1221\n",
      "Epoch 7, time 12.7422, train_loss: 1.0864, test_loss: 1.1196\n",
      "Epoch 8, time 13.9049, train_loss: 1.0659, test_loss: 1.1224\n",
      "Epoch 9, time 15.1185, train_loss: 1.0781, test_loss: 1.0670\n",
      "Epoch 10, time 16.3199, train_loss: 1.0399, test_loss: 1.0523\n",
      "Epoch 11, time 17.5448, train_loss: 1.0330, test_loss: 1.0468\n",
      "Epoch 12, time 18.7317, train_loss: 1.0171, test_loss: 0.9933\n",
      "Epoch 13, time 20.0932, train_loss: 0.9955, test_loss: 1.0055\n",
      "Epoch 14, time 22.5012, train_loss: 0.9796, test_loss: 0.9999\n",
      "Epoch 15, time 23.6409, train_loss: 1.0250, test_loss: 0.9865\n",
      "Epoch 16, time 24.7597, train_loss: 1.0195, test_loss: 1.0669\n",
      "Epoch 17, time 25.8471, train_loss: 1.0371, test_loss: 1.0445\n",
      "Epoch 18, time 27.0487, train_loss: 0.9694, test_loss: 0.9658\n",
      "Epoch 19, time 28.6038, train_loss: 0.9475, test_loss: 0.9838\n",
      "Epoch 20, time 29.7465, train_loss: 0.9232, test_loss: 0.9772\n",
      "Epoch 21, time 32.2807, train_loss: 0.9314, test_loss: 0.9621\n",
      "Epoch 22, time 33.6998, train_loss: 0.9453, test_loss: 0.9649\n",
      "Epoch 23, time 34.9816, train_loss: 0.9421, test_loss: 0.9780\n",
      "Epoch 24, time 36.2436, train_loss: 0.9339, test_loss: 0.9535\n",
      "Epoch 25, time 37.6035, train_loss: 0.8925, test_loss: 0.9386\n",
      "Epoch 26, time 38.7654, train_loss: 0.9145, test_loss: 0.9737\n",
      "Epoch 27, time 40.0518, train_loss: 0.9115, test_loss: 0.9869\n",
      "Epoch 28, time 41.2192, train_loss: 0.9135, test_loss: 0.9273\n",
      "Epoch 29, time 42.4800, train_loss: 0.9298, test_loss: 1.0001\n",
      "Epoch 30, time 43.6186, train_loss: 0.8991, test_loss: 0.9604\n",
      "Epoch 31, time 45.0120, train_loss: 0.8945, test_loss: 0.9531\n",
      "Epoch 32, time 46.2362, train_loss: 0.8781, test_loss: 0.9414\n",
      "Epoch 33, time 47.4415, train_loss: 0.8729, test_loss: 0.9320\n",
      "Epoch 34, time 48.7128, train_loss: 0.8622, test_loss: 1.0042\n",
      "Epoch 35, time 50.0755, train_loss: 0.9101, test_loss: 0.9628\n",
      "Epoch 36, time 54.3852, train_loss: 0.8680, test_loss: 0.9376\n",
      "Epoch 37, time 55.5624, train_loss: 0.8541, test_loss: 0.9675\n",
      "Epoch 38, time 56.8928, train_loss: 0.8481, test_loss: 0.9598\n",
      "Epoch 39, time 60.3022, train_loss: 0.8863, test_loss: 0.9983\n",
      "Epoch 40, time 62.7219, train_loss: 0.8438, test_loss: 0.9020\n",
      "Epoch 41, time 64.2611, train_loss: 0.8542, test_loss: 0.9322\n",
      "Epoch 42, time 65.5686, train_loss: 0.8715, test_loss: 1.0073\n",
      "Epoch 43, time 66.7891, train_loss: 0.8572, test_loss: 0.9308\n",
      "Epoch 44, time 69.1468, train_loss: 0.8264, test_loss: 0.9156\n",
      "Epoch 45, time 70.4490, train_loss: 0.8502, test_loss: 0.9470\n",
      "Epoch 46, time 71.6413, train_loss: 0.8380, test_loss: 0.9740\n",
      "Epoch 47, time 72.8839, train_loss: 0.8318, test_loss: 0.9567\n",
      "Epoch 48, time 74.1030, train_loss: 0.8098, test_loss: 0.9761\n",
      "Epoch 49, time 75.3432, train_loss: 0.8280, test_loss: 0.9598\n",
      "Epoch 50, time 76.5030, train_loss: 0.8118, test_loss: 0.9695\n",
      "Epoch 51, time 77.8768, train_loss: 0.8025, test_loss: 0.9874\n",
      "Epoch 52, time 80.5686, train_loss: 0.7982, test_loss: 0.9535\n",
      "Epoch 53, time 84.5263, train_loss: 0.7892, test_loss: 0.9530\n",
      "Epoch 54, time 87.5033, train_loss: 0.8232, test_loss: 0.9818\n",
      "Epoch 55, time 89.1081, train_loss: 0.8304, test_loss: 0.9551\n",
      "Epoch 56, time 94.9060, train_loss: 0.7813, test_loss: 0.9561\n",
      "Epoch 57, time 98.5810, train_loss: 0.7763, test_loss: 0.9405\n",
      "Epoch 58, time 104.5141, train_loss: 0.7794, test_loss: 0.9460\n",
      "Epoch 59, time 106.1408, train_loss: 0.7803, test_loss: 0.9790\n",
      "Epoch 60, time 108.3165, train_loss: 0.7864, test_loss: 0.9643\n",
      "Epoch 61, time 110.3909, train_loss: 0.7805, test_loss: 0.9441\n",
      "Epoch 62, time 111.7533, train_loss: 0.7818, test_loss: 0.9607\n",
      "Epoch 63, time 112.9660, train_loss: 0.7740, test_loss: 0.9580\n",
      "Epoch 64, time 115.3319, train_loss: 0.7703, test_loss: 0.9939\n",
      "Epoch 65, time 116.5634, train_loss: 0.7718, test_loss: 0.9462\n",
      "Epoch 66, time 117.8500, train_loss: 0.7492, test_loss: 0.9692\n",
      "Epoch 67, time 119.1546, train_loss: 0.7411, test_loss: 0.9823\n",
      "Epoch 68, time 120.4488, train_loss: 0.7442, test_loss: 0.9509\n",
      "Epoch 69, time 122.6949, train_loss: 0.7400, test_loss: 0.9581\n",
      "Epoch 70, time 124.2753, train_loss: 0.7467, test_loss: 0.9666\n",
      "Epoch 71, time 125.5667, train_loss: 0.7297, test_loss: 0.9658\n",
      "Epoch 72, time 126.7725, train_loss: 0.7377, test_loss: 1.0059\n",
      "Epoch 73, time 128.8551, train_loss: 0.7666, test_loss: 1.0286\n",
      "Epoch 74, time 130.2403, train_loss: 0.7673, test_loss: 0.9723\n",
      "Epoch 75, time 131.4488, train_loss: 0.7280, test_loss: 0.9627\n",
      "Epoch 76, time 132.6262, train_loss: 0.7325, test_loss: 0.9551\n",
      "Epoch 77, time 133.8212, train_loss: 0.7048, test_loss: 0.9807\n",
      "Epoch 78, time 135.1178, train_loss: 0.7293, test_loss: 1.0157\n",
      "Epoch 79, time 136.2997, train_loss: 0.7266, test_loss: 0.9504\n",
      "Epoch 80, time 137.4316, train_loss: 0.7159, test_loss: 0.9683\n",
      "Epoch 81, time 138.6701, train_loss: 0.6912, test_loss: 0.9980\n",
      "Epoch 82, time 139.9228, train_loss: 0.6865, test_loss: 0.9728\n",
      "Epoch 83, time 141.2213, train_loss: 0.6991, test_loss: 0.9890\n",
      "Epoch 84, time 142.5314, train_loss: 0.6977, test_loss: 0.9372\n",
      "Epoch 85, time 144.1237, train_loss: 0.6887, test_loss: 0.9891\n",
      "Epoch 86, time 146.4391, train_loss: 0.6895, test_loss: 0.9809\n",
      "Epoch 87, time 147.7781, train_loss: 0.7050, test_loss: 0.9707\n",
      "Epoch 88, time 149.1487, train_loss: 0.6930, test_loss: 0.9776\n",
      "Epoch 89, time 150.4168, train_loss: 0.6692, test_loss: 1.0021\n",
      "Epoch 90, time 152.7398, train_loss: 0.6687, test_loss: 0.9954\n",
      "Epoch 91, time 154.4717, train_loss: 0.7011, test_loss: 0.9616\n",
      "Epoch 92, time 155.6705, train_loss: 0.6631, test_loss: 0.9823\n",
      "Epoch 93, time 156.9043, train_loss: 0.6797, test_loss: 0.9657\n",
      "Epoch 94, time 158.3497, train_loss: 0.6777, test_loss: 0.9458\n",
      "Epoch 95, time 159.5841, train_loss: 0.6667, test_loss: 0.9462\n",
      "Epoch 96, time 161.0802, train_loss: 0.6705, test_loss: 1.0110\n",
      "Epoch 97, time 162.4129, train_loss: 0.6929, test_loss: 1.0093\n",
      "Epoch 98, time 163.6588, train_loss: 0.6581, test_loss: 0.9570\n",
      "Epoch 99, time 165.0121, train_loss: 0.6537, test_loss: 0.9873\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4483    | 0.7327    |\n",
    "| GT without edge features (only atom type)    | 0.6583   | 0.9095    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
