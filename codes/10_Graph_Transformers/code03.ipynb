{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra)\n",
    "\n",
    "### Xavier Bresson\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 0.6953s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types \n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node and edge features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature \n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=85, num_edges=182,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 0.6611],\n",
      "        [ 0.4881],\n",
      "        [-0.3908],\n",
      "        [ 0.2305],\n",
      "        [ 2.8032],\n",
      "        [-1.0106],\n",
      "        [-1.0700],\n",
      "        [-1.9760],\n",
      "        [-0.0457],\n",
      "        [ 1.1236]])\n",
      "batch_x: torch.Size([85])\n",
      "batch_pe: torch.Size([85, 3])\n",
      "batch_e: torch.Size([182])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design the class of GraphTransformer networks with edge features \n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WF): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WG): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbresson/miniconda3/envs/gnn_course/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "# class graph multi head attention layer  \n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "    \n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WF = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WG = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges): \n",
    "        # Compute bi-linear products with edge feature : q_i^T * diag(e_ij) * k_j \n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j, edges.data[] form ij\" \n",
    "        qikj = (edges.src['K'] * edges.data['E'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "        #qikj = (edges.src['K'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # NO edge feature\n",
    "        #qikj = ### YOUR CODE HERE, NO edge feature\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        # Compute edge feature : q_i^T * diag(e_ij) * k_j\n",
    "        eij = edges.src['K'] * edges.data['E'] * edges.dst['Q'] / torch.sqrt(torch.tensor(self.head_hidden_dim)) # e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        #eij = ### YOUR CODE HERE, size=(E,K,d')\n",
    "        edges.data['e'] = eij # update edge feature \n",
    "        return {'expij' : expij, 'vj' : vj} \n",
    "    \n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h} \n",
    "    \n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "        F = self.WF(h) # size=(N, d)\n",
    "        G = self.WG(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.ndata['F'] = F.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['G'] = G.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA \n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "    \n",
    "    \n",
    "# class GraphTransformer layer  \n",
    "class GraphTransformer_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        \n",
    "    def forward(self, g, h, e): \n",
    "        \n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm1e(e) # layer normalization, size=(N, d)\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp (h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP =# Design the class of GraphTransformer networks with edge features  self.dropout_e_mlp (e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "        \n",
    "        return h, e\n",
    "    \n",
    "    \n",
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ]) \n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)  \n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        \n",
    "    def forward(self, g, h, pe, e):\n",
    "        \n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # if PE used\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # input edge embedding\n",
    "        e = self.embedding_e(e) # size=(num_edges, hidden_dim)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)  \n",
    "        y = self.ln_h_final(mol_token)  \n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)    \n",
    "\n",
    "        return y    \n",
    "    \n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 799233 (0.80 million)\n",
      "Epoch 0, time 2.6667, train_loss: 1.3360, test_loss: 1.2837\n",
      "Epoch 1, time 5.0044, train_loss: 1.2404, test_loss: 1.0858\n",
      "Epoch 2, time 7.8705, train_loss: 1.0493, test_loss: 0.9973\n",
      "Epoch 3, time 10.1764, train_loss: 0.9930, test_loss: 1.0183\n",
      "Epoch 4, time 14.1055, train_loss: 0.9775, test_loss: 0.9911\n",
      "Epoch 5, time 18.8307, train_loss: 0.9419, test_loss: 0.9173\n",
      "Epoch 6, time 21.1271, train_loss: 0.9145, test_loss: 0.9313\n",
      "Epoch 7, time 27.2841, train_loss: 0.9075, test_loss: 0.8750\n",
      "Epoch 8, time 31.0738, train_loss: 0.8775, test_loss: 0.8465\n",
      "Epoch 9, time 34.0877, train_loss: 0.8644, test_loss: 0.9112\n",
      "Epoch 10, time 36.3976, train_loss: 0.8462, test_loss: 0.8051\n",
      "Epoch 11, time 38.6810, train_loss: 0.8343, test_loss: 0.8128\n",
      "Epoch 12, time 40.9917, train_loss: 0.8401, test_loss: 0.8371\n",
      "Epoch 13, time 43.3201, train_loss: 0.8209, test_loss: 0.8340\n",
      "Epoch 14, time 46.8183, train_loss: 0.7905, test_loss: 0.8224\n",
      "Epoch 15, time 50.2743, train_loss: 0.8122, test_loss: 0.8021\n",
      "Epoch 16, time 52.9253, train_loss: 0.8312, test_loss: 0.7993\n",
      "Epoch 17, time 59.7749, train_loss: 0.7727, test_loss: 0.7783\n",
      "Epoch 18, time 66.0919, train_loss: 0.8104, test_loss: 0.8027\n",
      "Epoch 19, time 69.1636, train_loss: 0.8391, test_loss: 0.8428\n",
      "Epoch 20, time 72.7529, train_loss: 0.7824, test_loss: 0.7880\n",
      "Epoch 21, time 75.0917, train_loss: 0.7583, test_loss: 0.8035\n",
      "Epoch 22, time 78.6316, train_loss: 0.7583, test_loss: 0.8070\n",
      "Epoch 23, time 81.3481, train_loss: 0.7302, test_loss: 0.7808\n",
      "Epoch 24, time 84.1348, train_loss: 0.7148, test_loss: 0.7363\n",
      "Epoch 25, time 90.0509, train_loss: 0.7006, test_loss: 0.7326\n",
      "Epoch 26, time 95.3614, train_loss: 0.7464, test_loss: 0.7914\n",
      "Epoch 27, time 97.6884, train_loss: 0.7346, test_loss: 0.7761\n",
      "Epoch 28, time 99.8071, train_loss: 0.6995, test_loss: 0.7495\n",
      "Epoch 29, time 101.8549, train_loss: 0.6789, test_loss: 0.7384\n",
      "Epoch 30, time 104.0051, train_loss: 0.7059, test_loss: 0.7623\n",
      "Epoch 31, time 107.2662, train_loss: 0.7048, test_loss: 0.7650\n",
      "Epoch 32, time 109.6513, train_loss: 0.6588, test_loss: 0.7240\n",
      "Epoch 33, time 111.7963, train_loss: 0.6751, test_loss: 0.7443\n",
      "Epoch 34, time 114.5939, train_loss: 0.6848, test_loss: 0.7478\n",
      "Epoch 35, time 118.6753, train_loss: 0.7165, test_loss: 0.7361\n",
      "Epoch 36, time 122.8858, train_loss: 0.6757, test_loss: 0.7345\n",
      "Epoch 37, time 127.1603, train_loss: 0.6462, test_loss: 0.7323\n",
      "Epoch 38, time 130.1483, train_loss: 0.6291, test_loss: 0.7431\n",
      "Epoch 39, time 133.3435, train_loss: 0.6331, test_loss: 0.7329\n",
      "Epoch 40, time 137.4508, train_loss: 0.6422, test_loss: 0.7065\n",
      "Epoch 41, time 140.5315, train_loss: 0.6639, test_loss: 0.7646\n",
      "Epoch 42, time 142.8059, train_loss: 0.6446, test_loss: 0.7187\n",
      "Epoch 43, time 145.1819, train_loss: 0.6373, test_loss: 0.7082\n",
      "Epoch 44, time 148.1091, train_loss: 0.6136, test_loss: 0.7177\n",
      "Epoch 45, time 153.3738, train_loss: 0.6052, test_loss: 0.7307\n",
      "Epoch 46, time 158.1925, train_loss: 0.6129, test_loss: 0.7370\n",
      "Epoch 47, time 160.7526, train_loss: 0.5950, test_loss: 0.7032\n",
      "Epoch 48, time 163.4030, train_loss: 0.5970, test_loss: 0.7239\n",
      "Epoch 49, time 167.0017, train_loss: 0.6002, test_loss: 0.7099\n",
      "Epoch 50, time 169.9824, train_loss: 0.5948, test_loss: 0.7493\n",
      "Epoch 51, time 172.5837, train_loss: 0.5944, test_loss: 0.7077\n",
      "Epoch 52, time 175.4149, train_loss: 0.5733, test_loss: 0.7171\n",
      "Epoch 53, time 178.0891, train_loss: 0.5839, test_loss: 0.7119\n",
      "Epoch 54, time 182.2854, train_loss: 0.5782, test_loss: 0.7323\n",
      "Epoch 55, time 185.1202, train_loss: 0.6023, test_loss: 0.7133\n",
      "Epoch 56, time 187.9891, train_loss: 0.5947, test_loss: 0.7455\n",
      "Epoch 57, time 190.6242, train_loss: 0.5636, test_loss: 0.7193\n",
      "Epoch 58, time 194.4404, train_loss: 0.5677, test_loss: 0.7104\n",
      "Epoch 59, time 198.8856, train_loss: 0.5563, test_loss: 0.7462\n",
      "Epoch 60, time 202.1911, train_loss: 0.5530, test_loss: 0.7090\n",
      "Epoch 61, time 204.9068, train_loss: 0.5463, test_loss: 0.6950\n",
      "Epoch 62, time 207.7418, train_loss: 0.5436, test_loss: 0.7218\n",
      "Epoch 63, time 209.8398, train_loss: 0.5353, test_loss: 0.6914\n",
      "Epoch 64, time 213.2441, train_loss: 0.5374, test_loss: 0.7167\n",
      "Epoch 65, time 215.9277, train_loss: 0.5330, test_loss: 0.6923\n",
      "Epoch 66, time 217.9496, train_loss: 0.5741, test_loss: 0.7395\n",
      "Epoch 67, time 220.4299, train_loss: 0.5444, test_loss: 0.7283\n",
      "Epoch 68, time 222.8647, train_loss: 0.5519, test_loss: 0.7089\n",
      "Epoch 69, time 226.5002, train_loss: 0.5507, test_loss: 0.7571\n",
      "Epoch 70, time 229.2128, train_loss: 0.5481, test_loss: 0.7347\n",
      "Epoch 71, time 232.0360, train_loss: 0.5295, test_loss: 0.7378\n",
      "Epoch 72, time 234.4859, train_loss: 0.5157, test_loss: 0.7112\n",
      "Epoch 73, time 237.0539, train_loss: 0.5014, test_loss: 0.7536\n",
      "Epoch 74, time 239.4242, train_loss: 0.5263, test_loss: 0.7515\n",
      "Epoch 75, time 248.4825, train_loss: 0.5367, test_loss: 0.7361\n",
      "Epoch 76, time 254.8613, train_loss: 0.5048, test_loss: 0.7172\n",
      "Epoch 77, time 258.9496, train_loss: 0.4833, test_loss: 0.6921\n",
      "Epoch 78, time 261.4406, train_loss: 0.5000, test_loss: 0.7011\n",
      "Epoch 79, time 263.9629, train_loss: 0.4840, test_loss: 0.7145\n",
      "Epoch 80, time 266.2878, train_loss: 0.5092, test_loss: 0.7633\n",
      "Epoch 81, time 268.3283, train_loss: 0.5009, test_loss: 0.6919\n",
      "Epoch 82, time 270.8619, train_loss: 0.4898, test_loss: 0.7162\n",
      "Epoch 83, time 272.9113, train_loss: 0.4807, test_loss: 0.7136\n",
      "Epoch 84, time 278.0841, train_loss: 0.5189, test_loss: 0.7502\n",
      "Epoch 85, time 280.6408, train_loss: 0.5310, test_loss: 0.7088\n",
      "Epoch 86, time 284.6371, train_loss: 0.4914, test_loss: 0.6997\n",
      "Epoch 87, time 288.1202, train_loss: 0.4644, test_loss: 0.7125\n",
      "Epoch 88, time 290.0822, train_loss: 0.4520, test_loss: 0.7182\n",
      "Epoch 89, time 291.9798, train_loss: 0.4678, test_loss: 0.7279\n",
      "Epoch 90, time 293.8576, train_loss: 0.4673, test_loss: 0.7213\n",
      "Epoch 91, time 296.0919, train_loss: 0.4754, test_loss: 0.7193\n",
      "Epoch 92, time 298.0914, train_loss: 0.4610, test_loss: 0.7246\n",
      "Epoch 93, time 300.2704, train_loss: 0.4650, test_loss: 0.7175\n",
      "Epoch 94, time 303.5278, train_loss: 0.4685, test_loss: 0.7252\n",
      "Epoch 95, time 307.0120, train_loss: 0.4363, test_loss: 0.7050\n",
      "Epoch 96, time 309.8543, train_loss: 0.4369, test_loss: 0.7192\n",
      "Epoch 97, time 314.2759, train_loss: 0.4399, test_loss: 0.7183\n",
      "Epoch 98, time 317.8600, train_loss: 0.4758, test_loss: 0.7584\n",
      "Epoch 99, time 320.1314, train_loss: 0.4483, test_loss: 0.7327\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# # save initial net for comparison\n",
    "# checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "# if not os.path.exists(checkpoint_dir):\n",
    "#     os.makedirs(checkpoint_dir)\n",
    "# time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "# #torch.save({'net_init': net.state_dict(),}, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + \"ZINC_\" + time_stamp ))\n",
    "# torch.save({'net_init': net.state_dict(),}, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + \"QM9_\" + time_stamp ))\n",
    "\n",
    "# # load initial net for comparison\n",
    "# checkpoint_file = \"checkpoint/checkpoint_QM9_23-05-07--12-43-01.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file)\n",
    "# net.load_state_dict(checkpoint['net_init'])\n",
    "# del checkpoint\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GT without edge features \n",
    "\n",
    "Node update equation: \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# class graph multi head attention layer  \n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "    \n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges): \n",
    "        # Compute the dot products q_i^T * k_j \n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\" \n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), , edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj} \n",
    "    \n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        #numerator = ### YOUR CODE HERE, sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        #denominator = ### YOUR CODE HERE, sum_j' exp_ij', size=(N,K,1)\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h} \n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA \n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "    \n",
    "    \n",
    "# class GraphTransformer layer  \n",
    "class GraphTransformer_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        \n",
    "    def forward(self, g, h): \n",
    "        \n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "        \n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    \n",
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ]) \n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)  \n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "        \n",
    "    def forward(self, g, h, pe):\n",
    "        \n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # input node embedding = positional embedding\n",
    "        # Compute the linear transformation of the positional embedding \n",
    "        # You may use \"nn.Linear(pos_enc_dim, hidden_dim)\"\n",
    "        #h = ### YOUR CODE HERE, Lap eigenvectors fecture, size=(num_nodes, hidden_dim)\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)  \n",
    "        y = self.ln_h_final(mol_token)  \n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)    \n",
    "\n",
    "        return y    \n",
    "\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 1.4011, train_loss: 1.3564, test_loss: 1.2784\n",
      "Epoch 1, time 3.4852, train_loss: 1.2334, test_loss: 1.2013\n",
      "Epoch 2, time 5.6746, train_loss: 1.1561, test_loss: 1.2390\n",
      "Epoch 3, time 7.0911, train_loss: 1.1447, test_loss: 1.1141\n",
      "Epoch 4, time 8.9700, train_loss: 1.0881, test_loss: 1.0865\n",
      "Epoch 5, time 11.2606, train_loss: 1.0652, test_loss: 1.0715\n",
      "Epoch 6, time 14.0763, train_loss: 1.0755, test_loss: 1.1274\n",
      "Epoch 7, time 17.1256, train_loss: 1.1085, test_loss: 1.1349\n",
      "Epoch 8, time 20.4406, train_loss: 1.0509, test_loss: 1.1272\n",
      "Epoch 9, time 24.3999, train_loss: 1.0220, test_loss: 1.0409\n",
      "Epoch 10, time 26.0907, train_loss: 0.9947, test_loss: 0.9920\n",
      "Epoch 11, time 27.5423, train_loss: 0.9879, test_loss: 0.9996\n",
      "Epoch 12, time 28.7339, train_loss: 0.9765, test_loss: 0.9598\n",
      "Epoch 13, time 30.0875, train_loss: 0.9557, test_loss: 0.9587\n",
      "Epoch 14, time 31.4361, train_loss: 0.9561, test_loss: 1.0158\n",
      "Epoch 15, time 32.7028, train_loss: 0.9514, test_loss: 0.9504\n",
      "Epoch 16, time 34.0532, train_loss: 0.9116, test_loss: 0.9430\n",
      "Epoch 17, time 35.2406, train_loss: 0.9365, test_loss: 0.9254\n",
      "Epoch 18, time 36.4078, train_loss: 0.9462, test_loss: 0.9769\n",
      "Epoch 19, time 37.7446, train_loss: 0.9561, test_loss: 0.9625\n",
      "Epoch 20, time 38.9130, train_loss: 0.9433, test_loss: 0.9347\n",
      "Epoch 21, time 40.1173, train_loss: 0.9126, test_loss: 0.9279\n",
      "Epoch 22, time 42.0617, train_loss: 0.9479, test_loss: 0.9009\n",
      "Epoch 23, time 43.5545, train_loss: 0.9153, test_loss: 0.9092\n",
      "Epoch 24, time 46.3117, train_loss: 0.9272, test_loss: 0.9992\n",
      "Epoch 25, time 47.9895, train_loss: 0.9257, test_loss: 0.9250\n",
      "Epoch 26, time 49.4369, train_loss: 0.9453, test_loss: 1.0142\n",
      "Epoch 27, time 50.7680, train_loss: 1.0068, test_loss: 0.9430\n",
      "Epoch 28, time 52.1379, train_loss: 0.9511, test_loss: 1.0542\n",
      "Epoch 29, time 56.4456, train_loss: 0.9129, test_loss: 0.9136\n",
      "Epoch 30, time 58.1435, train_loss: 0.9154, test_loss: 0.9926\n",
      "Epoch 31, time 59.6421, train_loss: 0.9174, test_loss: 0.9265\n",
      "Epoch 32, time 61.1432, train_loss: 0.8939, test_loss: 0.9002\n",
      "Epoch 33, time 62.7837, train_loss: 0.8886, test_loss: 0.9107\n",
      "Epoch 34, time 64.2400, train_loss: 0.9111, test_loss: 0.9393\n",
      "Epoch 35, time 65.4964, train_loss: 0.8880, test_loss: 0.9266\n",
      "Epoch 36, time 66.7759, train_loss: 0.8829, test_loss: 0.9108\n",
      "Epoch 37, time 68.2239, train_loss: 0.9146, test_loss: 0.9105\n",
      "Epoch 38, time 69.6559, train_loss: 0.8820, test_loss: 0.8979\n",
      "Epoch 39, time 70.9596, train_loss: 0.8597, test_loss: 0.8891\n",
      "Epoch 40, time 72.4938, train_loss: 0.8517, test_loss: 0.8887\n",
      "Epoch 41, time 73.7558, train_loss: 0.8838, test_loss: 0.8985\n",
      "Epoch 42, time 76.1446, train_loss: 0.8750, test_loss: 0.8945\n",
      "Epoch 43, time 77.7361, train_loss: 0.8576, test_loss: 0.9090\n",
      "Epoch 44, time 79.3780, train_loss: 0.8576, test_loss: 0.9314\n",
      "Epoch 45, time 80.9231, train_loss: 0.8442, test_loss: 0.8935\n",
      "Epoch 46, time 82.6205, train_loss: 0.8518, test_loss: 0.8724\n",
      "Epoch 47, time 88.7846, train_loss: 0.8498, test_loss: 0.8914\n",
      "Epoch 48, time 91.0664, train_loss: 0.8407, test_loss: 0.9476\n",
      "Epoch 49, time 96.8391, train_loss: 0.8754, test_loss: 0.8987\n",
      "Epoch 50, time 98.4753, train_loss: 0.8337, test_loss: 0.8813\n",
      "Epoch 51, time 100.2278, train_loss: 0.8266, test_loss: 0.8897\n",
      "Epoch 52, time 103.6018, train_loss: 0.8171, test_loss: 0.9331\n",
      "Epoch 53, time 106.9570, train_loss: 0.8368, test_loss: 0.9084\n",
      "Epoch 54, time 109.1270, train_loss: 0.8319, test_loss: 0.9615\n",
      "Epoch 55, time 110.9797, train_loss: 0.8366, test_loss: 0.9366\n",
      "Epoch 56, time 113.0404, train_loss: 0.8787, test_loss: 0.8767\n",
      "Epoch 57, time 114.5974, train_loss: 0.8231, test_loss: 0.9127\n",
      "Epoch 58, time 116.2068, train_loss: 0.8058, test_loss: 0.9233\n",
      "Epoch 59, time 120.3484, train_loss: 0.8029, test_loss: 0.8985\n",
      "Epoch 60, time 121.8477, train_loss: 0.7924, test_loss: 0.9222\n",
      "Epoch 61, time 123.6528, train_loss: 0.8035, test_loss: 0.8852\n",
      "Epoch 62, time 127.7471, train_loss: 0.7835, test_loss: 0.8881\n",
      "Epoch 63, time 129.0770, train_loss: 0.7812, test_loss: 0.8890\n",
      "Epoch 64, time 130.4371, train_loss: 0.7949, test_loss: 0.8827\n",
      "Epoch 65, time 131.9651, train_loss: 0.7803, test_loss: 0.8700\n",
      "Epoch 66, time 133.2163, train_loss: 0.7608, test_loss: 0.8995\n",
      "Epoch 67, time 134.7383, train_loss: 0.7902, test_loss: 1.0176\n",
      "Epoch 68, time 137.3217, train_loss: 0.8388, test_loss: 0.9067\n",
      "Epoch 69, time 139.5091, train_loss: 0.7642, test_loss: 0.9149\n",
      "Epoch 70, time 141.1218, train_loss: 0.8056, test_loss: 0.8913\n",
      "Epoch 71, time 142.6475, train_loss: 0.8052, test_loss: 0.8780\n",
      "Epoch 72, time 144.0588, train_loss: 0.7670, test_loss: 0.8932\n",
      "Epoch 73, time 145.2950, train_loss: 0.7797, test_loss: 0.8806\n",
      "Epoch 74, time 146.6504, train_loss: 0.7765, test_loss: 0.9139\n",
      "Epoch 75, time 148.9835, train_loss: 0.7663, test_loss: 0.8790\n",
      "Epoch 76, time 151.1717, train_loss: 0.7496, test_loss: 0.9001\n",
      "Epoch 77, time 152.4003, train_loss: 0.7326, test_loss: 0.8873\n",
      "Epoch 78, time 153.5314, train_loss: 0.7273, test_loss: 0.8984\n",
      "Epoch 79, time 156.1128, train_loss: 0.7274, test_loss: 0.8759\n",
      "Epoch 80, time 157.8673, train_loss: 0.7265, test_loss: 0.9192\n",
      "Epoch 81, time 159.0553, train_loss: 0.7136, test_loss: 0.8729\n",
      "Epoch 82, time 160.2087, train_loss: 0.7226, test_loss: 0.9171\n",
      "Epoch 83, time 162.4221, train_loss: 0.7173, test_loss: 0.9044\n",
      "Epoch 84, time 163.6803, train_loss: 0.6984, test_loss: 0.9164\n",
      "Epoch 85, time 165.3225, train_loss: 0.6987, test_loss: 0.9023\n",
      "Epoch 86, time 167.3413, train_loss: 0.6931, test_loss: 0.8983\n",
      "Epoch 87, time 168.5809, train_loss: 0.6981, test_loss: 0.9055\n",
      "Epoch 88, time 170.0413, train_loss: 0.7176, test_loss: 0.9043\n",
      "Epoch 89, time 171.2410, train_loss: 0.6904, test_loss: 0.8980\n",
      "Epoch 90, time 172.5599, train_loss: 0.6945, test_loss: 0.8896\n",
      "Epoch 91, time 173.8489, train_loss: 0.6890, test_loss: 0.9083\n",
      "Epoch 92, time 180.4285, train_loss: 0.6768, test_loss: 0.9105\n",
      "Epoch 93, time 182.8350, train_loss: 0.6728, test_loss: 0.8982\n",
      "Epoch 94, time 184.1543, train_loss: 0.6630, test_loss: 0.9384\n",
      "Epoch 95, time 185.5587, train_loss: 0.6725, test_loss: 0.9411\n",
      "Epoch 96, time 188.5203, train_loss: 0.6756, test_loss: 0.9134\n",
      "Epoch 97, time 189.7537, train_loss: 0.6670, test_loss: 0.9299\n",
      "Epoch 98, time 190.9531, train_loss: 0.6705, test_loss: 0.9240\n",
      "Epoch 99, time 192.2237, train_loss: 0.6583, test_loss: 0.9095\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# # save initial net for comparison\n",
    "# checkpoint_dir = os.path.join(\"checkpoint\")\n",
    "# if not os.path.exists(checkpoint_dir):\n",
    "#     os.makedirs(checkpoint_dir)\n",
    "# time_stamp = datetime.datetime.now().strftime(\"%y-%m-%d--%H-%M-%S\")\n",
    "# #torch.save({'net_init': net.state_dict(),}, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + \"ZINC_\" + time_stamp ))\n",
    "# torch.save({'net_init': net.state_dict(),}, '{}.pkl'.format(checkpoint_dir + \"/checkpoint_\" + \"QM9_\" + time_stamp ))\n",
    "\n",
    "# # load initial net for comparison\n",
    "# checkpoint_file = \"checkpoint/checkpoint_QM9_23-05-07--12-43-01.pkl\"\n",
    "# checkpoint = torch.load(checkpoint_file)\n",
    "# net.load_state_dict(checkpoint['net_init'])\n",
    "# del checkpoint\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4483    | 0.7327    |\n",
    "| GT without edge features (only atom type)    | 0.6583   | 0.9095    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
