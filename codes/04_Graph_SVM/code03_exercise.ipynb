{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph SVM\n",
    "\n",
    "## Lab 03 : Kernel/Non-Linear SVM -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/04_Graph_SVM'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook \n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import time\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_purity\n",
    "from lib.utils import compute_SVM\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import sklearn.metrics.pairwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linearly separable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_twomoons_kernelSVM.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "Cgt_train = mat['C_train_errors'] - 1; Cgt_train = Cgt_train.squeeze()\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "p1 = plt.subplot(121)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtrain[:,0], Xtrain[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_train, color=pyplot.jet())\n",
    "plt.title('Training Data with 25% ERRORS')\n",
    "p2 = plt.subplot(122)\n",
    "size_vertex_plot = 33\n",
    "plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=Cgt_test, color=pyplot.jet())\n",
    "plt.title('Test Data')\n",
    "#plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run soft-margin SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run soft-margin SVM\n",
    "\n",
    "# Compute linear kernel, L, Q\n",
    "Ker = Xtrain.dot(Xtrain.T)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(Xtrain.dot(Xtest.T))\n",
    "\n",
    "# Error parameter\n",
    "lamb = 0.1 \n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = 0.0\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha[alpha<0.0] = 0 # Projection on [0,+infty]\n",
    "    alpha[alpha>lamb] = lamb # Projection on [-infty,lamb]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        size_vertex_plot = 33\n",
    "        plt.figure(figsize=(12,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^Tx+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^Tx+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        #plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Calculate the distance of each pair data points and compute the Gaussian kernel\n",
    "\n",
    "Gaussian kernel is defined as : $K_{i,j} = \\exp({\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}})$ between a pair of data points $(i,j)$.\n",
    "  \n",
    "You may use function `sklearn.metrics.pairwise.pairwise_distances(X, Y, metric='euclidean', n_jobs=1)` to compute the euclidean distance between all vector pairs $\\|x_i - x_j\\|^2$.\n",
    "\n",
    "Hint: You may consider $\\sigma=0.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "# The Euclidean distance of pair training data points\n",
    "train_Ddist = sklearn.metrics.pairwise.pairwise_distances()\n",
    "\n",
    "# The Euclidean distance of pair data points between training data and testing data\n",
    "test_Ddist = sklearn.metrics.pairwise.pairwise_distances()\n",
    "\n",
    "# Compute Gaussian kernel\n",
    "sigma = \n",
    "sigma2 = sigma**2\n",
    "Ker = \n",
    "KXtest = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Evaluate the performance of Kernel SVM on non-linearly separable data with different error parameters. \n",
    "\n",
    "Can kernel SVM outperform soft-margin linear SVM on non-linearly separable data?\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "# Error parameter\n",
    "lamb = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kernel SVM\n",
    "\n",
    "# Compute Gaussian kernel, L, Q\n",
    "sigma = 0.5; sigma2 = sigma**2\n",
    "Ddist = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtrain, metric='euclidean', n_jobs=1)\n",
    "Ker = np.exp(- Ddist**2 / sigma2)\n",
    "Ddist = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtest, metric='euclidean', n_jobs=1)\n",
    "KXtest = np.exp(- Ddist**2 / sigma2)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(KXtest)\n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = 0.0\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha[alpha<0.0] = 0 # Projection on [0,+infty]\n",
    "    alpha[alpha>lamb] = lamb # Projection on [-infty,lamb]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Plot\n",
    "        size_vertex_plot = 33\n",
    "        plt.figure(figsize=(12,4))\n",
    "        p1 = plt.subplot(121)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=f_test, color=pyplot.jet())\n",
    "        plt.title('Score function $s(x)=w^T\\phi(x)+b$ \\n iter=' + str(k)+ ', diff_alpha=' + str(diff_alpha)[:7])\n",
    "        plt.colorbar()\n",
    "        p2 = plt.subplot(122)\n",
    "        plt.scatter(Xtest[:,0], Xtest[:,1], s=size_vertex_plot*np.ones(n), c=C_test, color=pyplot.jet())\n",
    "        plt.title('Classification function $f(x)=sign(w^T\\phi(x)+b)$\\n iter=' + str(k) + ', acc=' + str(accuracy_test)[:5])\n",
    "        #plt.tight_layout()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        if k<num_iter-1:\n",
    "            clear_output(wait=True)   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world graph of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "mat = scipy.io.loadmat('datasets/data_20news_50labels.mat')\n",
    "Xtrain = mat['Xtrain']\n",
    "l_train = mat['l'].squeeze()\n",
    "n = Xtrain.shape[0]\n",
    "d = Xtrain.shape[1]\n",
    "nc = len(np.unique(Cgt_train))\n",
    "print(n,d,nc)\n",
    "Xtest = mat['Xtest']\n",
    "Cgt_test = mat['Cgt_test'] - 1; Cgt_test = Cgt_test.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Run linear SVM on the real-world data \n",
    "\n",
    "Find the value `lamb` that maximizes the accuracy for the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "# Error parameter\n",
    "lamb = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear SVM\n",
    "\n",
    "# Compute Gaussian kernel, L, Q\n",
    "Ker = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtrain, metric='euclidean', n_jobs=1)\n",
    "KXtest = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtest, metric='euclidean', n_jobs=1)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(KXtest)\n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = 0.0\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha[alpha<0.0] = 0 # Projection on [0,+infty]\n",
    "    alpha[alpha>lamb] = lamb # Projection on [-infty,lamb]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Print\n",
    "        print('Linear SVM, iter, diff_alpha, acc :',str(k),str(diff_alpha)[:7],str(accuracy_test)[:5])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Evaluate the performance of kernel SVM on the real-world data \n",
    "\n",
    "Compare the results with soft-margin linear SVM. \n",
    "\n",
    "What are the implications of kernel SVM outperforming soft-margin linear SVM on real-world data?\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# Your code start\n",
    "############################################################################\n",
    "\n",
    "# Error parameter\n",
    "lamb = \n",
    "\n",
    "############################################################################\n",
    "# Your code end\n",
    "############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run kernel SVM\n",
    "\n",
    "# Compute Gaussian kernel, L, Q\n",
    "sigma = 0.5; sigma2 = sigma**2\n",
    "Ddist = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtrain, metric='euclidean', n_jobs=1)\n",
    "Ker = np.exp(- Ddist**2 / sigma2)\n",
    "Ddist = sklearn.metrics.pairwise.pairwise_distances(Xtrain, Xtest, metric='euclidean', n_jobs=1)\n",
    "KXtest = np.exp(- Ddist**2 / sigma2)\n",
    "l = l_train\n",
    "L = np.diag(l)\n",
    "Q = L.dot(Ker.dot(L))\n",
    "\n",
    "# Time steps\n",
    "tau_alpha = 10/ np.linalg.norm(Q,2)\n",
    "tau_beta = 0.1/ np.linalg.norm(L,2)\n",
    "\n",
    "# For conjuguate gradient\n",
    "Acg = tau_alpha* Q + np.eye(n)\n",
    "\n",
    "# Pre-compute J.K(Xtest) for test data\n",
    "LKXtest = L.dot(KXtest)\n",
    "\n",
    "# Initialization\n",
    "alpha = np.zeros([n])\n",
    "beta = 0.0\n",
    "alpha_old = alpha\n",
    "\n",
    "# Loop\n",
    "k = 0\n",
    "diff_alpha = 1e6\n",
    "num_iter = 201\n",
    "while (diff_alpha>1e-3) & (k<num_iter):\n",
    "    \n",
    "    # Update iteration\n",
    "    k += 1\n",
    "    \n",
    "    # Update alpha\n",
    "    # Approximate solution with conjuguate gradient\n",
    "    b0 = alpha + tau_alpha - tau_alpha* l* beta\n",
    "    alpha, _ = scipy.sparse.linalg.cg(Acg, b0, x0=alpha, tol=1e-3, maxiter=50)   \n",
    "    alpha[alpha<0.0] = 0 # Projection on [0,+infty]\n",
    "    alpha[alpha>lamb] = lamb # Projection on [-infty,lamb]\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + tau_beta* l.T.dot(alpha)\n",
    "    \n",
    "    # Stopping condition\n",
    "    diff_alpha = np.linalg.norm(alpha-alpha_old)\n",
    "    alpha_old = alpha\n",
    "    \n",
    "    # Plot\n",
    "    if not(k%10) or (diff_alpha<1e-3):\n",
    "           \n",
    "        # Indicator function of support vectors\n",
    "        idx = np.where( np.abs(alpha)>0.25* np.max(np.abs(alpha)) )\n",
    "        Isv = np.zeros([n]); Isv[idx] = 1\n",
    "        nb_sv = len(Isv.nonzero()[0])\n",
    "        \n",
    "        # Offset\n",
    "        if nb_sv > 1:\n",
    "            b = (Isv.T).dot( l - Ker.dot(L.dot(alpha)) )/ nb_sv\n",
    "        else:\n",
    "            b = 0\n",
    "            \n",
    "        # Continuous score function\n",
    "        f_test = alpha.T.dot(LKXtest) + b \n",
    "\n",
    "        # Binary classification function\n",
    "        C_test = np.sign(f_test) # decision function in {-1,1}\n",
    "        accuracy_test = compute_purity(0.5*(1+C_test),Cgt_test,nc) # 0.5*(1+C_test) in {0,1}\n",
    "\n",
    "        # Print\n",
    "        # print('iter, diff_alpha',str(k),str(diff_alpha)[:7])\n",
    "        # print('acc',str(accuracy_test)[:5])\n",
    "\n",
    "print('Kernel SVM  iter, diff_alpha :',str(k),str(diff_alpha)[:7])\n",
    "print('            acc :',str(accuracy_test)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
